{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(RooFitMP_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RooFitMP_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "\n",
    "from RooFitMP_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_1538069 = build_comb_df_split_timing_info('../rootbench/1538069.burrell.nikhef.nl.out')\n",
    "dfs_1538069 = load_result_file('../rootbench/1538069.burrell.nikhef.nl/RoofitMPworkspace_1549966927.json', match_y_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_timings_1538069 = dfs_1538069['BM_RooFit_MP_GradMinimizer_workspace_file']\n",
    "df_meta_1538069 = df_total_timings_1538069.drop(['real_time', 'real or ideal'], axis=1).dropna().set_index('benchmark_number', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_timings_1538069 = dfs_1538069['BM_RooFit_RooMinimizer_workspace_file']\n",
    "vanilla_t, vanilla_t_std = df_baseline_timings_1538069['real_time'].mean() / 1000, df_baseline_timings_1538069['real_time'].std() / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = combine_detailed_with_gbench_timings_by_name(df_total_timings_1538069,\n",
    "                                                   df_split_timings_1538069,\n",
    "                                                   {'update': 'update state',\n",
    "                                                    'gradient': 'gradient work',\n",
    "                                                    'terminate': 'terminate'},\n",
    "                                                   add_ideal=['gradient'])\n",
    "_g = sns.relplot(data=_df,\n",
    "                x='NumCPU', y='time [s]', style=\"real or ideal\",\n",
    "                hue='timing_type',\n",
    "                markers=True, err_style=\"bars\", legend='full', kind=\"line\")\n",
    "\n",
    "linestyle = {'color': 'black', 'lw': 0.7}\n",
    "_g.axes[0,0].axhline(vanilla_t, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t - vanilla_t_std, alpha=0.5, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t + vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timing_types = {\n",
    "    'update': 'update state',\n",
    "    'gradient': 'gradient work',\n",
    "    'terminate': 'terminate',\n",
    "    'partial derivatives': 'partial derivative'\n",
    "}\n",
    "_df = combine_detailed_with_gbench_timings_by_name(df_total_timings_1538069,\n",
    "                                                   df_split_timings_1538069,\n",
    "                                                   timing_types=_timing_types,\n",
    "                                                   add_ideal=['gradient'],\n",
    "                                                   exclude_from_rest=['partial derivatives'])\n",
    "_g = sns.relplot(data=_df,\n",
    "            x='NumCPU', y='time [s]', style=\"real or ideal\",\n",
    "            hue='timing_type',\n",
    "            markers=True, err_style=\"bars\", legend='full', kind=\"line\")\n",
    "\n",
    "linestyle = {'color': 'black', 'lw': 0.7}\n",
    "_g.axes[0,0].axhline(vanilla_t, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t - vanilla_t_std, alpha=0.5, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t + vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with optConst = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_1604382 = build_comb_df_split_timing_info('../rootbench/1604382.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_1604382 = load_result_file('../rootbench/1604382.burrell.nikhef.nl/RoofitMPworkspaceNoOptConst_1551699016.json', match_y_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_timings_1604382 = dfs_1604382['BM_RooFit_MP_GradMinimizer_workspace_file_noOptConst']\n",
    "df_meta_1604382 = df_total_timings_1604382.drop(['real_time', 'real or ideal'], axis=1).dropna().set_index('benchmark_number', drop=True)\n",
    "\n",
    "df_baseline_timings_1604382 = dfs_1604382['BM_RooFit_RooMinimizer_workspace_file_noOptConst']\n",
    "vanilla_t, vanilla_t_std = df_baseline_timings_1604382['real_time'].mean() / 1000, df_baseline_timings_1604382['real_time'].std() / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timing_types = {\n",
    "    'update': 'update state',\n",
    "    'gradient': 'gradient work',\n",
    "    'terminate': 'terminate',\n",
    "    'partial derivatives': 'partial derivative'\n",
    "}\n",
    "_df = combine_detailed_with_gbench_timings_by_name(df_total_timings_1604382,\n",
    "                                                   df_split_timings_1604382,\n",
    "                                                   timing_types=_timing_types,\n",
    "                                                   add_ideal=['gradient'],\n",
    "                                                   exclude_from_rest=['partial derivatives'])\n",
    "_g = sns.relplot(data=_df,\n",
    "            x='NumCPU', y='time [s]', style=\"real or ideal\",\n",
    "            hue='timing_type',\n",
    "            markers=True, err_style=\"bars\", legend='full', kind=\"line\")\n",
    "\n",
    "linestyle = {'color': 'black', 'lw': 0.7}\n",
    "_g.axes[0,0].axhline(vanilla_t, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t - vanilla_t_std, alpha=0.5, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t + vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large workspace run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_1604381 = build_comb_df_split_timing_info('../rootbench/1604381.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_1604381 = load_result_file('../rootbench/1604381.burrell.nikhef.nl/RoofitMPworkspace_1551694135.json', match_y_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that one failed after 4 runs, but let's see how that went anyway to get a better feeling for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df_split_timings_1604381[df_split_timings_1604381['timing_type'] == 'gradient work']\n",
    "_x = np.arange(len(_df))\n",
    "plt.bar(_x, _df['time [s]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df['time [s]'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More timings\n",
    "\n",
    "In the next benchmarks, we added a lot more timing output that needs to be incorporated into the analysis:\n",
    "\n",
    "- Line search timings: single lines starting with `line_search: `\n",
    "- update_real timings on queue process\n",
    "- update_real timings on worker processes\n",
    "- absolute time stamps (in nanoseconds since epoch) for:\n",
    "    + start migrad (this line has changed!)\n",
    "    + end migrad (same)\n",
    "    + for each worker: lines that contain either two or three stamps:\n",
    "        - time of ask for task and time of rejection\n",
    "        - time of ask for task, time of start, time of end of task\n",
    "    + maybe the update_real/update_state ones as well, if useful\n",
    "    \n",
    "As an additional book keeping complication, we need to run the large workspaces separately for different NumCPU parameters, both to speed up the runs (let them run on the cluster in parallel) and because we are currently getting crashes when running everything in one go; when running with 10 repeats per NumCPU the whole thing just stops after 4 repeats of the single-worker run; when running with 1 repeat it just crashes after the single-worker run (though it does write out the benchmark data to JSON, so that's promising for running the tasks separately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try-out: 32 core large workspace run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_1604381 = build_comb_df_split_timing_info('../rootbench/1633603.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, so, we're going to have to make adjustments, due to the new timings having been added.\n",
    "\n",
    "Let's paste the necessary functions back in here and adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_split_timing_info(fn):\n",
    "    \"\"\"\n",
    "    Group lines by benchmark iteration, starting from migrad until\n",
    "    after the forks have been terminated.\n",
    "    \"\"\"\n",
    "    with open(fn, 'r') as fh:\n",
    "        lines = fh.read().splitlines()\n",
    "\n",
    "    bm_iterations = []\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for ix, line in enumerate(lines):\n",
    "        if 'start migrad' in line:\n",
    "            if lines[ix - 1] == 'start migrad':  # sometimes 'start migrad' appears twice\n",
    "                start_indices.pop()\n",
    "            start_indices.append(ix)\n",
    "        elif 'terminate' in line:\n",
    "            end_indices.append(ix)\n",
    "\n",
    "    if len(start_indices) != len(end_indices):\n",
    "        raise Exception(f\"Number of start and end indices unequal (resp. {len(start_indices)} and {len(end_indices)})!\")\n",
    "\n",
    "    for ix in range(len(start_indices)):\n",
    "        bm_iterations.append(lines[start_indices[ix] + 1:end_indices[ix] + 1])\n",
    "\n",
    "    return bm_iterations\n",
    "\n",
    "\n",
    "def separate_partderi_job_time_lines(lines):\n",
    "    partial_derivatives = []\n",
    "    job_times = []\n",
    "    for line in lines:\n",
    "#         if line[:18] == '[#1] DEBUG: -- job':\n",
    "        if line[:9] == 'worker_id' or line[:24] == '[#0] DEBUG: -- worker_id':\n",
    "            partial_derivatives.append(line)\n",
    "        else:\n",
    "            job_times.append(line)\n",
    "    return partial_derivatives, job_times\n",
    "\n",
    "\n",
    "def group_timing_lines(bm_iteration_lines):\n",
    "    \"\"\"\n",
    "    Group lines (from one benchmark iteration) by gradient call,\n",
    "    specifying:\n",
    "    - Update time\n",
    "    - Gradient work time\n",
    "    - For all partial derivatives a sublist of all lines\n",
    "    Finally, the terminate time for the entire bm_iteration is also\n",
    "    returned (last line in the list).\n",
    "    \"\"\"\n",
    "    gradient_calls = []\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    # flag to check whether we are still in the same gradient call\n",
    "    in_block = False\n",
    "    for ix, line in enumerate(bm_iteration_lines[:-1]):  # -1: leave out terminate line\n",
    "        if not in_block and (line[:9] == 'worker_id' or line[:24] == '[#0] DEBUG: -- worker_id'):\n",
    "            start_indices.append(ix)\n",
    "            in_block = True\n",
    "        elif line[:12] == 'update_state' or line[:27] == '[#0] DEBUG: -- update_state':\n",
    "            end_indices.append(ix)\n",
    "            in_block = False\n",
    "\n",
    "    if len(start_indices) != len(end_indices):\n",
    "        raise Exception(f\"Number of start and end indices unequal (resp. {len(start_indices)} and {len(end_indices)})!\")\n",
    "        \n",
    "    for ix in range(len(start_indices)):\n",
    "        partial_derivatives, job_times = separate_partderi_job_time_lines(bm_iteration_lines[start_indices[ix]:end_indices[ix]])\n",
    "        gradient_calls.append({\n",
    "            'gradient_total': bm_iteration_lines[end_indices[ix]],\n",
    "            'partial_derivatives': partial_derivatives,\n",
    "            'job_times': job_times\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        terminate_line = bm_iteration_lines[-1]\n",
    "    except IndexError:\n",
    "        terminate_line = None\n",
    "\n",
    "    return gradient_calls, terminate_line\n",
    "\n",
    "\n",
    "def build_df_split_timing_run(timing_grouped_lines_list, terminate_line):\n",
    "    data = {'time [s]': [], 'timing_type': [], 'worker_id': [], 'task': []}\n",
    "\n",
    "    for gradient_call_timings in timing_grouped_lines_list:\n",
    "        words = gradient_call_timings['gradient_total'].split()\n",
    "\n",
    "        data['time [s]'].append(float(words[4][:-2]))\n",
    "        data['timing_type'].append('update state')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "        data['time [s]'].append(float(words[11][:-1]))\n",
    "        data['timing_type'].append('gradient work')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "        for partial_derivative_line in gradient_call_timings['partial_derivatives']:\n",
    "            words = partial_derivative_line.split()\n",
    "            try:\n",
    "                data['worker_id'].append(words[4][:-1])\n",
    "                data['task'].append(words[6][:-1])\n",
    "                data['time [s]'].append(float(words[10][:-1]))\n",
    "                data['timing_type'].append('partial derivative')\n",
    "            except ValueError as e:\n",
    "                print(words)\n",
    "                raise e\n",
    "            \n",
    "    words = terminate_line.split()\n",
    "    data['time [s]'].append(float(words[4][:-1]))\n",
    "    data['timing_type'].append('terminate')\n",
    "    data['worker_id'].append(None)\n",
    "    data['task'].append(None)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def build_dflist_split_timing_info(fn):\n",
    "    bm_iterations = extract_split_timing_info(fn)\n",
    "\n",
    "    dflist = []\n",
    "    for bm in bm_iterations:\n",
    "        grouped_lines, terminate_line = group_timing_lines(bm)\n",
    "        if terminate_line is not None:\n",
    "            dflist.append(build_df_split_timing_run(grouped_lines, terminate_line))\n",
    "\n",
    "    return dflist\n",
    "\n",
    "\n",
    "def build_comb_df_split_timing_info(fn):\n",
    "    dflist = build_dflist_split_timing_info(fn)\n",
    "\n",
    "    ix = 0\n",
    "    for df in dflist:\n",
    "        df_pardiff = df[df[\"timing_type\"] == \"partial derivative\"]\n",
    "        N_tasks = len(df_pardiff[\"task\"].unique())\n",
    "        N_gradients = len(df_pardiff) // N_tasks\n",
    "        gradient_indices = np.hstack(i * np.ones(N_tasks, dtype='int') for i in range(N_gradients))\n",
    "\n",
    "        df[\"gradient number\"] = pd.Series(dtype='Int64')\n",
    "        df.loc[df[\"timing_type\"] == \"partial derivative\", \"gradient number\"] = gradient_indices\n",
    "\n",
    "        df[\"benchmark_number\"] = ix\n",
    "        ix += 1\n",
    "\n",
    "    return pd.concat(dflist)\n",
    "\n",
    "# AhsF1415\n",
    "def combine_split_total_timings(df_total_timings, df_split_timings,\n",
    "                                calculate_rest=True, exclude_from_rest=[],\n",
    "                                add_ideal=[]):\n",
    "    df_meta = df_total_timings.drop(['real_time', 'real or ideal'], axis=1).dropna().set_index('benchmark_number', drop=True)\n",
    "\n",
    "    df_all_timings = df_total_timings.rename(columns={'real_time': 'time [s]'})\n",
    "    df_all_timings['time [s]'] /= 1000  # convert to seconds\n",
    "    df_all_timings['timing_type'] = 'total'\n",
    "\n",
    "    df_split_sum = {}\n",
    "    for name, df in df_split_timings.items():\n",
    "        df_split_sum[name] = df.groupby('benchmark_number').sum().join(df_meta, on='benchmark_number').reset_index()\n",
    "        df_split_sum[name]['real or ideal'] = 'real'\n",
    "        if name in add_ideal:\n",
    "            df_split_sum[name] = add_ideal_timings(df_split_sum[name], time_col='time [s]')\n",
    "        df_split_sum[name]['timing_type'] = name\n",
    "\n",
    "    # note: sort sorts the *columns* if they are not aligned, nothing happens with the column data itself\n",
    "    df_all_timings = pd.concat([df_all_timings, ] + list(df_split_sum.values()), sort=True)\n",
    "\n",
    "    if calculate_rest:\n",
    "        rest_time = df_all_timings[(df_all_timings['timing_type'] == 'total') & (df_all_timings['real or ideal'] == 'real')].set_index('benchmark_number')['time [s]']\n",
    "        for name, df in df_split_sum.items():\n",
    "            if name not in exclude_from_rest:\n",
    "                rest_time = rest_time - df.set_index('benchmark_number')['time [s]']\n",
    "\n",
    "        df_rest_time = rest_time.to_frame().join(df_meta, on='benchmark_number').reset_index()\n",
    "        df_rest_time['timing_type'] = 'rest'\n",
    "        df_rest_time['real or ideal'] = 'real'\n",
    "\n",
    "        # note: sort sorts the *columns* if they are not aligned, nothing happens with the column data itself\n",
    "        df_all_timings = df_all_timings.append(df_rest_time, sort=True)\n",
    "\n",
    "    return df_all_timings\n",
    "\n",
    "\n",
    "def combine_detailed_with_gbench_timings_by_name(df_gbench, df_detailed, timing_types={}, **kwargs):\n",
    "    detailed_selection = {}\n",
    "    if len(timing_types) == 0:\n",
    "        raise Exception(\"Please give some timing_types, otherwise this function is pointless.\")\n",
    "    for name, timing_type in timing_types.items():\n",
    "        detailed_selection[name] = df_detailed[df_detailed['timing_type'] == timing_type].drop('timing_type', axis=1)\n",
    "    return combine_split_total_timings(df_gbench, detailed_selection, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_1633603 = build_comb_df_split_timing_info('../rootbench/1633603.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that works, now to combine them with the json timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_split_timings_1633594__603 = {}\n",
    "for i in range(1633594, 1633604):\n",
    "    dfs_split_timings_1633594__603[i] = build_comb_df_split_timing_info(f'../rootbench/{i}.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the json timings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_1633594__603 = {}\n",
    "for i in range(1633594, 1633604):\n",
    "    json_file = glob.glob(f'../rootbench/{i}.burrell.nikhef.nl/RoofitMPworkspaceNumCPUInConfigFile_*.json')\n",
    "    if len(json_file) == 1:\n",
    "        dfs_1633594__603[i] = RooFitMP_analysis.load_result_file(json_file[0], plot_results=False)\n",
    "    else:\n",
    "        print(json_file)\n",
    "        raise Exception(\"whups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_total_timings_1633594__603 = {}\n",
    "nums = list(range(1, 9)) + [16, 32]\n",
    "num_from_run = {}\n",
    "for ix, run in enumerate(range(1633594, 1633604)):\n",
    "    num_from_run[run] = nums[ix]\n",
    "\n",
    "for run, df in dfs_1633594__603.items():\n",
    "    dfc = df['BM_RooFit_MP_GradMinimizer_workspace_file_NumCPUInConfigFile'].copy()\n",
    "    dfc['NumCPU'] = num_from_run[run]\n",
    "    dfs_total_timings_1633594__603[run] = dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine them per run, because otherwise the benchmark_numbers won't match (they only count within a run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_baseline_timings_1633594__603 = pd.concat([df['BM_RooFit_RooMinimizer_workspace_file_NumCPUInConfigFile'] for df in dfs_1633594__603])\n",
    "# vanilla_t, vanilla_t_std = df_baseline_timings_1633594__603['real_time'].mean() / 1000, df_baseline_timings_1633594__603['real_time'].std() / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timing_types = {\n",
    "    'update': 'update state',\n",
    "    'gradient': 'gradient work',\n",
    "    'terminate': 'terminate',\n",
    "#     'partial derivatives': 'partial derivative'\n",
    "}\n",
    "\n",
    "_comb = {}\n",
    "for i in range(1633594, 1633604):\n",
    "    _comb[i] = combine_detailed_with_gbench_timings_by_name(dfs_total_timings_1633594__603[i],\n",
    "                                                   dfs_split_timings_1633594__603[i],\n",
    "                                                   timing_types=_timing_types,\n",
    "                                                   add_ideal=['gradient'],\n",
    "                                                   exclude_from_rest=['partial derivatives'])\n",
    "\n",
    "_df = pd.concat(_comb.values())\n",
    "\n",
    "_g = sns.relplot(data=_df,\n",
    "            x='NumCPU', y='time [s]', style=\"real or ideal\",\n",
    "            hue='timing_type',\n",
    "            markers=True, err_style=\"bars\", legend='full', kind=\"line\")\n",
    "_g.fig.set_size_inches(10,6)\n",
    "_g.axes[0,0].set_xlim((0,33))\n",
    "# linestyle = {'color': 'black', 'lw': 0.7}\n",
    "# _g.axes[0,0].axhline(vanilla_t, **linestyle)\n",
    "# _g.axes[0,0].axhline(vanilla_t - vanilla_t_std, alpha=0.5, **linestyle)\n",
    "# _g.axes[0,0].axhline(vanilla_t + vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rather big rest term now! It's linear though...\n",
    "\n",
    "Could this be the cost of the extra prints we now introduced all over the place? Especially the update_state ones are a lot.\n",
    "\n",
    "Only one way to really find out: run again without all those added prints. We're not using them yet anyway.\n",
    "\n",
    "... But we can do a quick estimate as well. A flush costs about 10 microseconds according to the interwebs. The output files of the runs (which each contain 3 repeats) have the following number of lines:\n",
    "- 32 workers: 5309080\n",
    "- 16 workers: 2647773\n",
    "- 8 workers: 1419130\n",
    "- 1 worker: 345466\n",
    "\n",
    "This means that the flushing overhead per minimization is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'32': 5309080/3/100000,\n",
    " '16': 2647773/3/100000,\n",
    " '8': 1419130/3/100000,\n",
    " '1': 345466/3/100000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that's not the dominant term in the rest term, although it may explain the slight upward trend at the end, though we could hardly call that significant.\n",
    "\n",
    "Let's look at the line search timings as well. There is one line_search print after each gradient in these runs, that's fine. There's also a whole bunch of other line_search prints at the end of the output file which belong to the RooMinimizer run, so we shouldn't include those. We will then only modify the function above to take the line_search lines directly below a gradient output block.\n",
    "\n",
    "Also, we'll add the queue and worker process update_real timings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_timing_lines(bm_iteration_lines):\n",
    "    \"\"\"\n",
    "    Group lines (from one benchmark iteration) by gradient call,\n",
    "    specifying:\n",
    "    - Update time on master\n",
    "    - update_real times on queue and workers\n",
    "    - Gradient work time\n",
    "    - For all partial derivatives a sublist of all lines\n",
    "    - After each gradient block, there may be line_search times,\n",
    "      these are also included as a sublist\n",
    "    Finally, the terminate time for the entire bm_iteration is also\n",
    "    returned (last line in the list).\n",
    "    \"\"\"\n",
    "    gradient_calls = []\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    line_search_lines = []\n",
    "    update_queue = []\n",
    "    update_worker = []\n",
    "    # flag to check whether we are still in the same gradient call\n",
    "    in_block = False\n",
    "    for ix, line in enumerate(bm_iteration_lines[:-1]):  # -1: leave out terminate line\n",
    "        if not in_block and (line[:9] == 'worker_id' or line[:24] == '[#0] DEBUG: -- worker_id'):\n",
    "            start_indices.append(ix)\n",
    "            in_block = True\n",
    "        elif 'update_state' in line:\n",
    "            end_indices.append(ix)\n",
    "            in_block = False\n",
    "        # the rest has nothing to do with the gradient call block, so we don't touch in_block there:\n",
    "        elif 'line_search' in line:\n",
    "            line_search_lines.append(line)\n",
    "        elif 'update_real on queue' in line:\n",
    "            update_queue.append(line)\n",
    "        elif 'update_real on worker' in line:\n",
    "            update_worker.append(line)\n",
    "    \n",
    "    if len(start_indices) != len(end_indices):\n",
    "        raise Exception(f\"Number of start and end indices unequal (resp. {len(start_indices)} and {len(end_indices)})!\")\n",
    "        \n",
    "    for ix in range(len(start_indices)):\n",
    "        partial_derivatives, job_times = separate_partderi_job_time_lines(bm_iteration_lines[start_indices[ix]:end_indices[ix]])\n",
    "        gradient_calls.append({\n",
    "            'gradient_total': bm_iteration_lines[end_indices[ix]],\n",
    "            'partial_derivatives': partial_derivatives,\n",
    "            'job_times': job_times\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        terminate_line = bm_iteration_lines[-1]\n",
    "    except IndexError:\n",
    "        terminate_line = None\n",
    "\n",
    "    return gradient_calls, line_search_lines, update_queue, update_worker, terminate_line\n",
    "\n",
    "\n",
    "def build_df_split_timing_run(timing_grouped_lines_list, line_search_lines, update_queue, update_worker, terminate_line):\n",
    "    data = {'time [s]': [], 'timing_type': [], 'worker_id': [], 'task': []}\n",
    "\n",
    "    for gradient_call_timings in timing_grouped_lines_list:\n",
    "        words = gradient_call_timings['gradient_total'].split()\n",
    "\n",
    "        data['time [s]'].append(float(words[4][:-2]))\n",
    "        data['timing_type'].append('update state')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "        data['time [s]'].append(float(words[11][:-1]))\n",
    "        data['timing_type'].append('gradient work')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "        \n",
    "        for partial_derivative_line in gradient_call_timings['partial_derivatives']:\n",
    "            words = partial_derivative_line.split()\n",
    "            try:\n",
    "                data['worker_id'].append(words[4][:-1])\n",
    "                data['task'].append(words[6][:-1])\n",
    "                data['time [s]'].append(float(words[10][:-1]))\n",
    "                data['timing_type'].append('partial derivative')\n",
    "            except ValueError as e:\n",
    "                print(words)\n",
    "                raise e\n",
    "\n",
    "    for line_search_line in line_search_lines:\n",
    "        words = line_search_line.split()\n",
    "        data['time [s]'].append(float(words[1][:-1]))\n",
    "        data['timing_type'].append('line search')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "    for line in update_queue:\n",
    "        words = line.split()\n",
    "        data['time [s]'].append(float(words[6][:-1]))\n",
    "        data['timing_type'].append('update_real on queue')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "    for line in update_worker:\n",
    "        words = line.split()\n",
    "        data['time [s]'].append(float(words[7][:-1]))\n",
    "        data['timing_type'].append('update_real on worker')\n",
    "        data['worker_id'].append(int(words[6][:-1]))\n",
    "        data['task'].append(None)\n",
    "\n",
    "    words = terminate_line.split()\n",
    "    data['time [s]'].append(float(words[4][:-1]))\n",
    "    data['timing_type'].append('terminate')\n",
    "    data['worker_id'].append(None)\n",
    "    data['task'].append(None)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def build_dflist_split_timing_info(fn):\n",
    "    bm_iterations = extract_split_timing_info(fn)\n",
    "\n",
    "    dflist = []\n",
    "    for bm in bm_iterations:\n",
    "        grouped_lines, line_search_lines, update_queue, update_worker, terminate_line = group_timing_lines(bm)\n",
    "        if terminate_line is not None:\n",
    "            dflist.append(build_df_split_timing_run(grouped_lines, line_search_lines, update_queue, update_worker, terminate_line))\n",
    "\n",
    "    return dflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_split_timings_1633594__603_v2 = {}\n",
    "for i in range(1633594, 1633604):\n",
    "    dfs_split_timings_1633594__603_v2[i] = build_comb_df_split_timing_info(f'../rootbench/{i}.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timing_types = {\n",
    "    'update': 'update state',\n",
    "    'gradient': 'gradient work',\n",
    "    'terminate': 'terminate',\n",
    "    'line search': 'line search',\n",
    "    'update_real on queue': 'update_real on queue',\n",
    "    'update_real on worker': 'update_real on worker'\n",
    "#     'partial derivatives': 'partial derivative'\n",
    "}\n",
    "\n",
    "add_ideal_cols = ['gradient']\n",
    "\n",
    "_comb = {}\n",
    "for i in range(1633594, 1633604):\n",
    "    _comb[i] = combine_detailed_with_gbench_timings_by_name(dfs_total_timings_1633594__603[i],\n",
    "                                                   dfs_split_timings_1633594__603_v2[i],\n",
    "                                                   timing_types=_timing_types,\n",
    "                                                   exclude_from_rest=['partial derivatives',\n",
    "                                                                      'update_real on queue',\n",
    "                                                                      'update_real on worker'])\n",
    "\n",
    "_df = pd.concat(_comb.values())\n",
    "for col in add_ideal_cols:\n",
    "    df_ideal = RooFitMP_analysis.add_ideal_timings(_df[_df['timing_type'] == col],\n",
    "                                                   time_col='time [s]', return_ideal=True, ideal_stat='mean')\n",
    "    df_ideal['timing_type'] = col\n",
    "    _df = _df.append(df_ideal, sort=True)\n",
    "\n",
    "_g = sns.relplot(data=_df,\n",
    "            x='NumCPU', y='time [s]', style=\"real or ideal\",\n",
    "            hue='timing_type',\n",
    "            markers=True, err_style=\"bars\", legend='full', kind=\"line\")\n",
    "_g.fig.set_size_inches(16,8)\n",
    "_g.axes[0,0].set_xlim((0,33))\n",
    "# linestyle = {'color': 'black', 'lw': 0.7}\n",
    "# _g.axes[0,0].axhline(vanilla_t, **linestyle)\n",
    "# _g.axes[0,0].axhline(vanilla_t - vanilla_t_std, alpha=0.5, **linestyle)\n",
    "# _g.axes[0,0].axhline(vanilla_t + vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the line search is not insignificant, but also not the whole story.\n",
    "\n",
    "The update_real on the queue also grows a bit, but nothing too shocking.\n",
    "\n",
    "The update_real on the workers is not really useful as a total aggregate, since we're really only interested in the time that each worker individually \"wastes\", i.e. the time between when the master thinks it's done updating (but then the queue and workers still have to process it) and when the worker starts to do actual work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, another possibility for the rest term is actually the constant term optimization. We should measure this as well, because if indeed it is so big, it might become interesting to turn it off, or at least set it to 1 instead of 2. The latter option may give a factor 2 slower runs in single core mode, but if it can scale down due to parallelism, it might still become faster in the end!\n",
    "\n",
    "That still doesn't explain the discrepancy in real vs ideal of the gradient timing though. For that we really need to dive into the absolute timings to find out whether there are significant delays between jobs on the workers.\n",
    "\n",
    "A simple first tally of some of the output may already tell us some things. For instance, let's look at how much workers spend asking for work when there's nothing left:\n",
    "\n",
    "```sh\n",
    "for i in {594..603}; do no_work=$(grep \"no work\" 1633${i}.burrell.nikhef.nl.out | wc -l); job_done=$(grep \"job done\" 1633${i}.burrell.nikhef.nl.out | wc -l); echo \"$i: $job_done / $no_work\"; done\n",
    "```\n",
    "\n",
    "The result for these runs is:\n",
    "```\n",
    "594:    34512 /      178\n",
    "595:    34512 /      586\n",
    "596:    34512 /     6083\n",
    "597:    34512 /    33391\n",
    "598:    34512 /    32171\n",
    "599:    34512 /    63563\n",
    "600:    34512 /    87326\n",
    "601:    34512 /   107505\n",
    "602:    34512 /   231764\n",
    "603:    34512 /   684300\n",
    "```\n",
    "\n",
    "That's quite the rise! To see what kind of impact this has exactly, we'd have to dig deeper, though. It may be pretty much harmless, since all it should do is cause a delay in the queue loop for processing actually useful worker messages, but the useful workers are probably not producing useful results every nanosecond, so small delays in their processing may not be that important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timestamps\n",
    "\n",
    "Let's do one more modification round to also read in the absolute timestamps. We'll first leave out the timestamps of update_real, since we're more interested in the delays between start and work, between work and end and especially between jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_split_timing_info(fn):\n",
    "    \"\"\"\n",
    "    Group lines by benchmark iteration, starting from migrad until\n",
    "    after the forks have been terminated.\n",
    "    \"\"\"\n",
    "    with open(fn, 'r') as fh:\n",
    "        lines = fh.read().splitlines()\n",
    "\n",
    "    bm_iterations = []\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for ix, line in enumerate(lines):\n",
    "        if 'start migrad' in line:\n",
    "            if 'start migrad' in lines[ix - 1]:  # sometimes 'start migrad' appears twice\n",
    "                start_indices.pop()\n",
    "            start_indices.append(ix)\n",
    "        elif 'terminate: ' in line:\n",
    "            end_indices.append(ix)\n",
    "\n",
    "    if len(start_indices) != len(end_indices):\n",
    "        raise Exception(f\"Number of start and end indices unequal (resp. {len(start_indices)} and {len(end_indices)})!\")\n",
    "\n",
    "    for ix in range(len(start_indices)):\n",
    "        bm_iterations.append(lines[start_indices[ix]:end_indices[ix] + 1])\n",
    "\n",
    "    return bm_iterations\n",
    "\n",
    "\n",
    "def group_timing_lines(bm_iteration_lines):\n",
    "    \"\"\"\n",
    "    Group lines (from one benchmark iteration) by gradient call,\n",
    "    specifying:\n",
    "    - Update time on master\n",
    "    - update_real times on queue and workers\n",
    "    - Gradient work time\n",
    "    - For all partial derivatives a sublist of all lines\n",
    "    - After each gradient block, there may be line_search times,\n",
    "      these are also included as a sublist\n",
    "    Finally, the terminate time for the entire bm_iteration is also\n",
    "    returned (last line in the list).\n",
    "    \n",
    "    In each gradient, also timestamps are printed. These are not\n",
    "    further subdivided in this function, but are output as part of\n",
    "    the `gradient_calls` list for further processing elsewhere.\n",
    "    \"\"\"\n",
    "    gradient_calls = []\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    line_search_lines = []\n",
    "    update_queue = []\n",
    "    update_worker = []\n",
    "    # flag to check whether we are still in the same gradient call\n",
    "    in_block = False\n",
    "    for ix, line in enumerate(bm_iteration_lines[:-1]):  # -1: leave out terminate line\n",
    "        if not in_block and (line[:9] == 'worker_id' or line[:24] == '[#0] DEBUG: -- worker_id'):\n",
    "            start_indices.append(ix)\n",
    "            in_block = True\n",
    "        elif 'update_state' in line:\n",
    "            end_indices.append(ix)\n",
    "            in_block = False\n",
    "        # the rest has nothing to do with the gradient call block, so we don't touch in_block there:\n",
    "        elif 'line_search' in line:\n",
    "            line_search_lines.append(line)\n",
    "        elif 'update_real on queue' in line:\n",
    "            update_queue.append(line)\n",
    "        elif 'update_real on worker' in line:\n",
    "            update_worker.append(line)\n",
    "        elif 'start migrad' in line:\n",
    "            start_migrad_line = line\n",
    "        elif 'end migrad' in line:\n",
    "            end_migrad_line = line\n",
    "    \n",
    "    if len(start_indices) != len(end_indices):\n",
    "        raise Exception(f\"Number of start and end indices unequal (resp. {len(start_indices)} and {len(end_indices)})!\")\n",
    "        \n",
    "    for ix in range(len(start_indices)):\n",
    "        partial_derivatives, timestamps = separate_partderi_job_time_lines(bm_iteration_lines[start_indices[ix]:end_indices[ix]])\n",
    "        gradient_calls.append({\n",
    "            'gradient_total': bm_iteration_lines[end_indices[ix]],\n",
    "            'partial_derivatives': partial_derivatives,\n",
    "            'timestamps': timestamps\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        terminate_line = bm_iteration_lines[-1]\n",
    "    except IndexError:\n",
    "        terminate_line = None\n",
    "\n",
    "    return gradient_calls, line_search_lines, update_queue, update_worker, terminate_line, start_migrad_line, end_migrad_line\n",
    "    \n",
    "\n",
    "def build_df_stamps(grouped_lines, start_migrad_line, end_migrad_line):\n",
    "    data = {'timestamp': [], 'stamp_type': [], 'worker_id': []}\n",
    "\n",
    "    words = start_migrad_line.split()\n",
    "    data['timestamp'].append(int(words[6]))\n",
    "    data['stamp_type'].append('start migrad')\n",
    "    data['worker_id'].append(None)\n",
    "\n",
    "    NumCPU = int(words[10])\n",
    "\n",
    "    words = end_migrad_line.split()\n",
    "    data['timestamp'].append(int(words[6]))\n",
    "    data['stamp_type'].append('end migrad')\n",
    "    data['worker_id'].append(None)\n",
    "\n",
    "    for gradient_group in grouped_lines:\n",
    "        for line in gradient_group['timestamps']:\n",
    "            if 'no work' in line:\n",
    "                words = line.split()\n",
    "                data['worker_id'].append(int(words[6]))\n",
    "                data['timestamp'].append(int(words[9]))\n",
    "                data['stamp_type'].append('no job - asked')\n",
    "\n",
    "                data['worker_id'].append(int(words[6]))\n",
    "                data['timestamp'].append(int(words[14]))\n",
    "                data['stamp_type'].append('no job - denied')\n",
    "            elif 'job done' in line:\n",
    "                words = line.split()\n",
    "                data['worker_id'].append(int(words[6]))\n",
    "                data['timestamp'].append(int(words[9][:-1]))\n",
    "                data['stamp_type'].append('job done - asked')\n",
    "\n",
    "                data['worker_id'].append(int(words[6]))\n",
    "                data['timestamp'].append(int(words[12]))\n",
    "                data['stamp_type'].append('job done - started')\n",
    "\n",
    "                data['worker_id'].append(int(words[6]))\n",
    "                data['timestamp'].append(int(words[16]))\n",
    "                data['stamp_type'].append('job done - finished')\n",
    "            elif 'update_real' in line:\n",
    "                # discard for now\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"got a weird line:\\n\" + line)\n",
    "\n",
    "    return pd.DataFrame(data), NumCPU\n",
    "\n",
    "\n",
    "def build_dflist_split_timing_info(fn, extract_fcn=extract_split_timing_info):\n",
    "    bm_iterations = extract_fcn(fn)\n",
    "\n",
    "    dflist = []\n",
    "    dflist_stamps = []\n",
    "    NumCPU_list = []\n",
    "    for bm in bm_iterations:\n",
    "        grouped_lines, line_search_lines, update_queue, update_worker, terminate_line, start_migrad_line, end_migrad_line = group_timing_lines(bm)\n",
    "        if terminate_line is not None:\n",
    "            dflist.append(build_df_split_timing_run(grouped_lines, line_search_lines, update_queue, update_worker, terminate_line))\n",
    "\n",
    "            df_stamps, NumCPU = build_df_stamps(grouped_lines, start_migrad_line, end_migrad_line)\n",
    "            dflist_stamps.append(df_stamps)\n",
    "            NumCPU_list.append(NumCPU)\n",
    "\n",
    "    return dflist, dflist_stamps, NumCPU_list\n",
    "\n",
    "\n",
    "def build_comb_df_split_timing_info(fn, extract_fcn=extract_split_timing_info):\n",
    "    dflist, dflist_stamps, NumCPU_list = build_dflist_split_timing_info(fn, extract_fcn=extract_fcn)\n",
    "\n",
    "    for ix, df in enumerate(dflist):\n",
    "        df_pardiff = df[df[\"timing_type\"] == \"partial derivative\"]\n",
    "        N_tasks = len(df_pardiff[\"task\"].unique())\n",
    "        N_gradients = len(df_pardiff) // N_tasks\n",
    "        gradient_indices = np.hstack(i * np.ones(N_tasks, dtype='int') for i in range(N_gradients))\n",
    "\n",
    "        df[\"gradient number\"] = pd.Series(dtype='Int64')\n",
    "        df.loc[df[\"timing_type\"] == \"partial derivative\", \"gradient number\"] = gradient_indices\n",
    "        \n",
    "        dflist_stamps[ix][\"gradient number\"] = pd.Series(dtype='Int64')\n",
    "        dflist_stamps[ix].loc[dflist_stamps[ix][\"stamp_type\"] == \"job done - asked\", \"gradient number\"] = gradient_indices\n",
    "        dflist_stamps[ix].loc[dflist_stamps[ix][\"stamp_type\"] == \"job done - started\", \"gradient number\"] = gradient_indices\n",
    "        dflist_stamps[ix].loc[dflist_stamps[ix][\"stamp_type\"] == \"job done - finished\", \"gradient number\"] = gradient_indices\n",
    "\n",
    "        df[\"benchmark_number\"] = ix\n",
    "        dflist_stamps[ix][\"benchmark_number\"] = ix\n",
    "\n",
    "    # assuming the stamps are ordered properly, which I'm pretty sure is correct,\n",
    "    # we can do ffill:\n",
    "    df_stamps = pd.concat(dflist_stamps)\n",
    "    df_stamps.loc[~df_stamps['stamp_type'].str.contains('migrad'), 'gradient number'] = df_stamps.loc[~df_stamps['stamp_type'].str.contains('migrad'), 'gradient number'].fillna(method='ffill')\n",
    "        \n",
    "    return pd.concat(dflist), df_stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_1633594_v3, df_stamps_1633594 = build_comb_df_split_timing_info('../rootbench/1633594.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stamps_1633594[df_stamps_1633594['stamp_type'].str.contains('no job')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACAT 2019\n",
    "\n",
    "Some quick runs for ACAT 2019 that requires custom analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acat19_out(fn):\n",
    "    \"\"\"\n",
    "    Just single migrad runs, so no need for further splitting by run.\n",
    "    Does add dummy terminate line, because the other functions expect\n",
    "    this. Also dummy start and end migrad lines.\n",
    "    \"\"\"\n",
    "    with open(fn, 'r') as fh:\n",
    "        lines = fh.read().splitlines()\n",
    "    print(lines[-1])\n",
    "    lines = ['[#0] DEBUG: -- start migrad at 0 with NumCPU = 0'] + lines\n",
    "    lines.append('[#0] DEBUG: -- end migrad at 0')\n",
    "    lines.append('[#0] DEBUG: -- terminate: 0.0s')    \n",
    "    return [lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_acat19_1552415151 = '/Users/pbos/projects/apcocsm/code/acat19_1552415151.out'\n",
    "\n",
    "df_split_timings_acat19_1552415151, df_stamps_acat19_1552415151 = build_comb_df_split_timing_info(fn_acat19_1552415151,\n",
    "                                                                                 extract_fcn=load_acat19_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_acat19_1552415151.groupby('timing_type')['time [s]'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "579.161-536.84-15.586-1.225684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good, at least the rest term is constant. This is important, because in reality, this fit will take about an hour, so 25 seconds are acceptable overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_1633594_v3.groupby(['benchmark_number', 'timing_type'])['time [s]'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "536.840400/216.742200, 15.586356/5.496111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the other terms scale by a factor of about 2.5 to 3, while the rest term remains constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RooMinimizer timings for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_baseline_timings_1633594__603 = {}\n",
    "\n",
    "for run, df in dfs_1633594__603.items():\n",
    "    dfc = df['BM_RooFit_RooMinimizer_workspace_file_NumCPUInConfigFile'].copy()\n",
    "    dfc['NumCPU'] = 1\n",
    "    dfs_baseline_timings_1633594__603[run] = dfc\n",
    "\n",
    "df_baseline_timings_1633594__603 = pd.concat(dfs_baseline_timings_1633594__603.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_t, vanilla_t_std = df_baseline_timings_1633594__603['real_time'].mean() / 1000, df_baseline_timings_1633594__603['real_time'].std() / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timing_types = {\n",
    "    'update': 'update state',\n",
    "    'gradient': 'gradient work',\n",
    "    'terminate': 'terminate',\n",
    "    'line search': 'line search',\n",
    "#     'update_real on queue': 'update_real on queue',\n",
    "#     'update_real on worker': 'update_real on worker'\n",
    "#     'partial derivatives': 'partial derivative'\n",
    "}\n",
    "\n",
    "add_ideal_cols = ['gradient']\n",
    "\n",
    "_comb = {}\n",
    "for i in range(1633594, 1633604):\n",
    "    _comb[i] = combine_detailed_with_gbench_timings_by_name(dfs_total_timings_1633594__603[i],\n",
    "                                                   dfs_split_timings_1633594__603_v2[i],\n",
    "                                                   timing_types=_timing_types,\n",
    "                                                   exclude_from_rest=['partial derivatives',\n",
    "                                                                      'update_real on queue',\n",
    "                                                                      'update_real on worker'])\n",
    "\n",
    "_df = pd.concat(_comb.values())\n",
    "# for col in add_ideal_cols:\n",
    "#     df_ideal = RooFitMP_analysis.add_ideal_timings(_df[_df['timing_type'] == col],\n",
    "#                                                    time_col='time [s]', return_ideal=True, ideal_stat='mean')\n",
    "#     df_ideal['timing_type'] = col\n",
    "#     _df = _df.append(df_ideal, sort=True)\n",
    "\n",
    "# _g = sns.relplot(data=_df,\n",
    "#             x='NumCPU', y='time [s]', style=\"real or ideal\",\n",
    "#             hue='timing_type',\n",
    "#             markers=True,\n",
    "#                  err_style=\"bars\",\n",
    "#                  legend='full', kind=\"line\")\n",
    "_g = sns.catplot(data=_df, \n",
    "            x='NumCPU', y='time [s]', style=\"real or ideal\",\n",
    "            hue='timing_type',\n",
    "#             markers=True,\n",
    "#                  err_style=\"bars\",\n",
    "                 legend='full', kind=\"point\")\n",
    "_g.fig.set_size_inches(16,8)\n",
    "# _g.axes[0,0].set_xlim((0,33))\n",
    "\n",
    "linestyle = {'color': 'black', 'lw': 0.7}\n",
    "_g.axes[0,0].axhline(vanilla_t, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t - vanilla_t_std, alpha=0.5, **linestyle)\n",
    "_g.axes[0,0].axhline(vanilla_t + vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dfs_total_timings_1633594__603[1633594]['real_time'] / 1000).mean(), vanilla_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(14, 8))\n",
    "\n",
    "_timing_types = ['gradient', 'line search', 'update', 'terminate', 'rest']\n",
    "\n",
    "colors = plt.cm.get_cmap('tab10', 10)\n",
    "\n",
    "first_nc = True\n",
    "for nc in _df['NumCPU'].unique():\n",
    "    prev_time = 0\n",
    "    for ix_ttype, ttype in enumerate(_timing_types):\n",
    "        time_mu  = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "        time_std = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].std()\n",
    "        if first_nc:\n",
    "            label = ttype\n",
    "        else:\n",
    "            label = \"\"\n",
    "        ax.bar(str(nc), time_mu, bottom=prev_time, color=colors(ix_ttype), label=label)\n",
    "        prev_time += float(time_mu)\n",
    "    first_nc = False\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate speed-up\n",
    "W.r.t. mean time of single core run. **Not w.r.t. old RooMinimizer timing** because for that we don't measure all the specific timing types, only total time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_speedup = {'NumCPU': [], 'timing_type': [], 'speedup': []}\n",
    "\n",
    "for ttype in _df['timing_type'].unique():\n",
    "    mean_single_core_time = _df[(_df['NumCPU'] == 1) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "    for nc in _df['NumCPU'].unique():\n",
    "        mean_time = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "        _df_speedup['NumCPU'].append(nc)\n",
    "        _df_speedup['timing_type'].append(ttype)\n",
    "        _df_speedup['speedup'].append(mean_single_core_time / mean_time)\n",
    "\n",
    "_df_speedup = pd.DataFrame(_df_speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid'):\n",
    "    _g = sns.catplot(data=_df_speedup, x='NumCPU', y='speedup', hue='timing_type', kind='bar')\n",
    "    _g.fig.set_size_inches(14,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633594\n",
    "_st_b0 = _st[_st['benchmark_number'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stamps = _st[_st['stamp_type'] == 'start migrad']\n",
    "start_stamps\n",
    "# _st[~_st['stamp_type'].str.contains('migrad')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_stamps = _st[_st['stamp_type'] == 'end migrad']\n",
    "end_stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_st0 = _st_b0[_st_b0['stamp_type'] == 'start migrad']['timestamp'].iloc[0]\n",
    "end_st0   = _st_b0[_st_b0['stamp_type'] == 'end migrad']['timestamp'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(end_st0 - start_st0)/1.e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st_b0[~_st_b0['stamp_type'].str.contains('migrad')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get two kinds of worker-loop overhead out of the \"job done\" stamps: \"explicit\" overhead in the time between asked and started and \"implicit\" overhead in the time between a finished job and the asking of a next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st_b0_jd = _st_b0[_st_b0['stamp_type'].str.contains('job done')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st_b0_jd_ask = _st_b0_jd[_st_b0_jd['stamp_type'].str.contains('asked')]\n",
    "_st_b0_jd_sta = _st_b0_jd[_st_b0_jd['stamp_type'].str.contains('started')]\n",
    "_st_b0_jd_fin = _st_b0_jd[_st_b0_jd['stamp_type'].str.contains('finished')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_st_b0_jd_sta.reset_index()['timestamp'] - _st_b0_jd_ask.reset_index()['timestamp']).sum()/1.e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_st_b0_jd_ask.iloc[1:].reset_index()['timestamp'] - _st_b0_jd_fin.iloc[:-1].reset_index()['timestamp']).sum()/1.e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this sum together with the actual partial derivatives to the total gradient time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_split = dfs_split_timings_1633594__603_v2[1633594]\n",
    "_pd_time = _df_split[(_df_split[\"timing_type\"] == \"partial derivative\")\n",
    "                     & (_df_split[\"benchmark_number\"] == 0)]['time [s]'].sum()\n",
    "_grad_time = _df_split[(_df_split[\"timing_type\"] == \"gradient work\")\n",
    "                       & (_df_split[\"benchmark_number\"] == 0)]['time [s]'].sum()\n",
    "_grad_time, _pd_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, it's too much... wait, probably the times between gradients are longer, so they should be filtered out first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impl_overhead = 0\n",
    "for g in _st_b0_jd_ask['gradient number'].unique():\n",
    "    _ask_g = _st_b0_jd_ask[_st_b0_jd_ask['gradient number'] == g]\n",
    "    _fin_g = _st_b0_jd_fin[_st_b0_jd_fin['gradient number'] == g]\n",
    "    impl_overhead += (_ask_g.iloc[1:].reset_index()['timestamp'] - _fin_g.iloc[:-1].reset_index()['timestamp']).sum()/1.e9\n",
    "impl_overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "216.74220000000003 - (0.779148892 + 1.015019575 + 212.982649869)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that makes more sense. The remaining ~2 seconds could be any number of things, like communication delays with the queue, or delays caused by update_real.\n",
    "\n",
    "# TODO\n",
    "- Also check the wait time from job rejections.\n",
    "- Include delays from update_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of parallel task times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_split = dfs_split_timings_1633594__603_v2[1633594]\n",
    "_pdtime = _df_split[_df_split['timing_type'] == 'partial derivative']['time [s]']\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(14,8))\n",
    "_g = sns.distplot(_pdtime, kde=False)#, ax=ax)\n",
    "_g.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACAT 2019 talk plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = combine_detailed_with_gbench_timings_by_name(df_total_timings_1538069,\n",
    "                                                   df_split_timings_1538069,\n",
    "                                                   {'update': 'update state',\n",
    "                                                    'gradient': 'gradient work',\n",
    "                                                    'terminate': 'terminate'})\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(14, 8), dpi=200)\n",
    "\n",
    "_timing_types = ['gradient', 'update', 'terminate', 'rest']\n",
    "\n",
    "colors = plt.cm.get_cmap('tab10', 10)\n",
    "\n",
    "lighten = lambda c: min(0.3*c, 1)\n",
    "\n",
    "first_nc = True\n",
    "for nc in _df['NumCPU'].unique():\n",
    "    prev_time = 0\n",
    "    for ix_ttype, ttype in enumerate(_timing_types):\n",
    "        time_mu  = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "        time_std = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].std()\n",
    "        if first_nc:\n",
    "            label = ttype\n",
    "        else:\n",
    "            label = \"\"\n",
    "        color = colors(ix_ttype)\n",
    "        ecolor = (lighten(color[0]), lighten(color[1]), lighten(color[2]), 0.8)\n",
    "        ax.bar(str(nc), time_mu, bottom=prev_time, color=color, label=label,\n",
    "               yerr=time_std, ecolor=ecolor, capsize=5)\n",
    "        prev_time += float(time_mu)\n",
    "    first_nc = False\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xlabel('workers')\n",
    "ax.set_ylabel('time [s]')\n",
    "\n",
    "_vanilla_t, _vanilla_t_std = df_baseline_timings_1538069['real_time'].mean() / 1000, df_baseline_timings_1538069['real_time'].std() / 1000\n",
    "\n",
    "linestyle = {'color': 'black', 'lw': 0.7}\n",
    "ax.axhline(_vanilla_t, **linestyle)\n",
    "ax.axhline(_vanilla_t - _vanilla_t_std, alpha=0.5, **linestyle)\n",
    "ax.axhline(_vanilla_t + _vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_speedup = {'NumCPU': [], 'timing type': [], 'mean speedup': []}\n",
    "\n",
    "ttypes = _df['timing_type'].unique()\n",
    "\n",
    "for ttype in ttypes:\n",
    "    mean_single_core_time = _df[(_df['NumCPU'] == 1) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "    for nc in _df['NumCPU'].unique():\n",
    "        mean_time = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "        _df_speedup['NumCPU'].append(nc)\n",
    "        _df_speedup['timing type'].append(ttype)\n",
    "        _df_speedup['mean speedup'].append(mean_single_core_time / mean_time)\n",
    "\n",
    "_df_speedup = pd.DataFrame(_df_speedup)\n",
    "\n",
    "_g = sns.catplot(data=_df_speedup, x='NumCPU', y='mean speedup', hue='timing type', kind='point', palette='tab10', hue_order=_timing_types + ['total'])\n",
    "_g.fig.set_size_inches(11,8)\n",
    "_g.fig.set_dpi(200)\n",
    "_g.axes[0,0].set_xlabel('workers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timing_types = {\n",
    "    'update': 'update state',\n",
    "    'gradient': 'gradient work',\n",
    "    'terminate': 'terminate',\n",
    "    'line search': 'line search',\n",
    "}\n",
    "\n",
    "add_ideal_cols = ['gradient']\n",
    "\n",
    "_comb = {}\n",
    "for i in range(1633594, 1633604):\n",
    "    _comb[i] = combine_detailed_with_gbench_timings_by_name(dfs_total_timings_1633594__603[i],\n",
    "                                                   dfs_split_timings_1633594__603_v2[i],\n",
    "                                                   timing_types=_timing_types,\n",
    "                                                   exclude_from_rest=['partial derivatives',\n",
    "                                                                      'update_real on queue',\n",
    "                                                                      'update_real on worker'])\n",
    "\n",
    "_df = pd.concat(_comb.values())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(14, 8), dpi=200)\n",
    "\n",
    "_timing_types = ['gradient', 'line search', 'update', 'terminate', 'rest']\n",
    "\n",
    "colors = plt.cm.get_cmap('tab10', 10)\n",
    "\n",
    "lighten = lambda c: min(0.3*c, 1)\n",
    "\n",
    "first_nc = True\n",
    "for nc in _df['NumCPU'].unique():\n",
    "    prev_time = 0\n",
    "    for ix_ttype, ttype in enumerate(_timing_types):\n",
    "        time_mu  = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "        time_std = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].std()\n",
    "        if first_nc:\n",
    "            label = ttype\n",
    "        else:\n",
    "            label = \"\"\n",
    "        color = colors(ix_ttype)\n",
    "        ecolor = (lighten(color[0]), lighten(color[1]), lighten(color[2]), 0.8)\n",
    "        ax.bar(str(nc), time_mu, bottom=prev_time, color=color, label=label,\n",
    "               yerr=time_std, ecolor=ecolor, capsize=5)\n",
    "        prev_time += float(time_mu)\n",
    "    first_nc = False\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xlabel('workers')\n",
    "ax.set_ylabel('time [s]')\n",
    "\n",
    "# _vanilla_t, _vanilla_t_std = df_baseline_timings_1633594__603['real_time'].mean() / 1000, df_baseline_timings_1633594__603['real_time'].std() / 1000\n",
    "\n",
    "# linestyle = {'color': 'black', 'lw': 0.7}\n",
    "# ax.axhline(_vanilla_t, **linestyle)\n",
    "# ax.axhline(_vanilla_t - _vanilla_t_std, alpha=0.5, **linestyle)\n",
    "# ax.axhline(_vanilla_t + _vanilla_t_std, alpha=0.5, **linestyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_speedup = {'NumCPU': [], 'timing type': [], 'speedup': []}\n",
    "\n",
    "ttypes = _df['timing_type'].unique()\n",
    "\n",
    "for ttype in ttypes:\n",
    "    mean_single_core_time = _df[(_df['NumCPU'] == 1) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "    for nc in _df['NumCPU'].unique():\n",
    "#         mean_time = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]'].mean()\n",
    "        times = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == ttype)]['time [s]']\n",
    "        for time in times:\n",
    "            _df_speedup['NumCPU'].append(nc)\n",
    "            _df_speedup['timing type'].append(ttype)\n",
    "            _df_speedup['speedup'].append(mean_single_core_time / float(time))\n",
    "\n",
    "_df_speedup = pd.DataFrame(_df_speedup)\n",
    "\n",
    "_g = sns.catplot(data=_df_speedup, x='NumCPU', y='speedup', hue='timing type',\n",
    "                 kind='point', palette='tab10', hue_order=_timing_types + ['total'])\n",
    "_g.fig.set_size_inches(11,8)\n",
    "_g.fig.set_dpi(200)\n",
    "_g.axes[0,0].set_xlabel('workers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrapolate total time to longer run times: 10 minutes, 1 hour, 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the actual run\n",
    "total_runtime = _df[(_df['NumCPU'] == 1) & (_df['timing_type'] == 'total')]['time [s]'].mean()  # seconds\n",
    "gradient = _df[(_df['NumCPU'] == 1) & (_df['timing_type'] == 'gradient')]['time [s]'].mean()\n",
    "total_overhead = total_runtime - gradient\n",
    "\n",
    "_df_total_times = _df[(_df['timing_type'] == 'total')]\n",
    "_df_total_times['timing_type'] = 'total (~4min)'\n",
    "\n",
    "# extrapolate gradient timings\n",
    "_df_extrapolate_10min = {'NumCPU': [], 'timing_type': [], 'time [s]': []}\n",
    "extrap_factor_10min = (10 * 60 - total_overhead) / gradient\n",
    "for nc in _df['NumCPU'].unique():\n",
    "    times = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == 'gradient')]['time [s]']\n",
    "    for time in times:\n",
    "        _df_extrapolate_10min['NumCPU'].append(nc)\n",
    "        _df_extrapolate_10min['timing_type'].append('total (10min)')\n",
    "        _df_extrapolate_10min['time [s]'].append(float(time) * extrap_factor_10min + total_overhead)\n",
    "\n",
    "_df_extrapolate_10min = pd.DataFrame(_df_extrapolate_10min)\n",
    "\n",
    "\n",
    "_df_extrapolate_1h = {'NumCPU': [], 'timing_type': [], 'time [s]': []}\n",
    "extrap_factor_1h = (60 * 60 - total_overhead) / gradient\n",
    "for nc in _df['NumCPU'].unique():\n",
    "    times = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == 'gradient')]['time [s]']\n",
    "    for time in times:\n",
    "        _df_extrapolate_1h['NumCPU'].append(nc)\n",
    "        _df_extrapolate_1h['timing_type'].append('total (1h)')\n",
    "        _df_extrapolate_1h['time [s]'].append(float(time) * extrap_factor_1h + total_overhead)\n",
    "\n",
    "_df_extrapolate_1h = pd.DataFrame(_df_extrapolate_1h)\n",
    "\n",
    "_df_extrapolate_2h = {'NumCPU': [], 'timing_type': [], 'time [s]': []}\n",
    "extrap_factor_2h = (2 * 60 * 60 - total_overhead) / gradient\n",
    "for nc in _df['NumCPU'].unique():\n",
    "    times = _df[(_df['NumCPU'] == nc) & (_df['timing_type'] == 'gradient')]['time [s]']\n",
    "    for time in times:\n",
    "        _df_extrapolate_2h['NumCPU'].append(nc)\n",
    "        _df_extrapolate_2h['timing_type'].append('total (2h)')\n",
    "        _df_extrapolate_2h['time [s]'].append(float(time) * extrap_factor_2h + total_overhead)\n",
    "\n",
    "_df_extrapolate_2h = pd.DataFrame(_df_extrapolate_2h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_extr_speedup = {'NumCPU': [], 'timing type': [], 'speedup': []}\n",
    "\n",
    "for extr_time, _df_extr in {'true run': _df_total_times,\n",
    "                            '10min': _df_extrapolate_10min,\n",
    "                            '1h': _df_extrapolate_1h,\n",
    "                            '2h': _df_extrapolate_2h,\n",
    "                            }.items():\n",
    "    ttypes = _df_extr['timing_type'].unique()\n",
    "\n",
    "    for ttype in ttypes:\n",
    "        mean_single_core_time = _df_extr[(_df_extr['NumCPU'] == 1) & (_df_extr['timing_type'] == ttype)]['time [s]'].mean()\n",
    "        for nc in _df_extr['NumCPU'].unique():\n",
    "            times = _df_extr[(_df_extr['NumCPU'] == nc) & (_df_extr['timing_type'] == ttype)]['time [s]']\n",
    "            for time in times:\n",
    "                _df_extr_speedup['NumCPU'].append(nc)\n",
    "                _df_extr_speedup['timing type'].append(ttype)\n",
    "                _df_extr_speedup['speedup'].append(mean_single_core_time / float(time))\n",
    "\n",
    "_df_extr_speedup = pd.DataFrame(_df_extr_speedup)\n",
    "\n",
    "_g = sns.catplot(data=_df_extr_speedup, x='NumCPU', y='speedup', hue='timing type',\n",
    "                 kind='point', palette='tab10')\n",
    "_g.fig.set_size_inches(11,8)\n",
    "_g.fig.set_dpi(200)\n",
    "_g.axes[0,0].set_xlabel('workers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, talk done. Back to:\n",
    "\n",
    "# Back to back to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633594"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to just plot the timeline to visualize the load on the worker during the whole run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sta = _st_b0_jd_sta['timestamp'] - start_st0\n",
    "_fin = _st_b0_jd_fin['timestamp'] - start_st0\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 3))\n",
    "\n",
    "for ix in range(len(_sta)):\n",
    "# for ix in range(2000):\n",
    "    ax.barh('worker 0', _fin.iloc[ix] - _sta.iloc[ix], left=_sta.iloc[ix],\n",
    "            color='red', linewidth=0)\n",
    "\n",
    "ax.set_xlim((0, end_st0 - start_st0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_fin), len(_sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goddamn, that takes forever to complete. Seems like matplotlib cannot handle too much bars at the same time.\n",
    "\n",
    "But ok, no shockers here.\n",
    "\n",
    "Now for a multi-worker run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_1633603 = build_comb_df_split_timing_info('../rootbench/1633603.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_1633602 = build_comb_df_split_timing_info('../rootbench/1633602.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_1633601 = build_comb_df_split_timing_info('../rootbench/1633601.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bench_nr = (df_stamps_1633601['benchmark_number'] == 0)\n",
    "\n",
    "_start_st = df_stamps_1633601[_bench_nr\n",
    "                              & (df_stamps_1633601['stamp_type'] == 'start migrad')\n",
    "                             ]['timestamp'].iloc[0]\n",
    "_end_st = df_stamps_1633601[_bench_nr\n",
    "                              & (df_stamps_1633601['stamp_type'] == 'end migrad')\n",
    "                             ]['timestamp'].iloc[0]\n",
    "\n",
    "_sta = df_stamps_1633601[_bench_nr\n",
    "                         & (df_stamps_1633601['stamp_type'].str.contains('started'))\n",
    "                        ]#['timestamp'] - _start_st\n",
    "_fin = df_stamps_1633601[_bench_nr\n",
    "                         & (df_stamps_1633601['stamp_type'].str.contains('finished'))\n",
    "                        ]#['timestamp'] - _start_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_sta), len(_fin), _end_st, _start_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "ax.set_ylabel('worker')\n",
    "ax.set_xlim((0, _end_st - _start_st))\n",
    "\n",
    "batches = int(len(_sta)/1000) + 1  # get aruond matplotlib limits?\n",
    "\n",
    "for batch in range(batches):\n",
    "    for ix in range(batch * 1000, (batch + 1) * 1000):\n",
    "        worker = _fin.iloc[ix][\"worker_id\"]\n",
    "        ax.barh(worker,\n",
    "                _fin.iloc[ix][\"timestamp\"] - _sta.iloc[ix][\"timestamp\"],\n",
    "                left=_sta.iloc[ix][\"timestamp\"] - _start_st,\n",
    "                color='red', linewidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('qt5Agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "\n",
    "batches = int(len(_sta)/batch_size) + 1  # get aruond matplotlib limits?\n",
    "\n",
    "for batch in range(batches):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "    ax.set_ylabel('worker')\n",
    "    ax.set_xlim((0, _end_st - _start_st))\n",
    "    \n",
    "    for ix in range(min(batch * batch_size, len(_sta)),\n",
    "                    min((batch + 1) * batch_size, len(_sta))):\n",
    "        worker = _fin.iloc[ix][\"worker_id\"]\n",
    "        ax.barh(worker,\n",
    "                _fin.iloc[ix][\"timestamp\"] - _sta.iloc[ix][\"timestamp\"],\n",
    "                left=_sta.iloc[ix][\"timestamp\"] - _start_st,\n",
    "                color='red', linewidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad_nr in df_stamps_1633601['gradient number'].unique():\n",
    "    _grad_nr = (df_stamps_1633601['gradient number'] == grad_nr)\n",
    "\n",
    "    _sta = df_stamps_1633601[_bench_nr & _grad_nr\n",
    "                             & (df_stamps_1633601['stamp_type'].str.contains('started'))\n",
    "                            ]#['timestamp'] - _start_st\n",
    "    _fin = df_stamps_1633601[_bench_nr & _grad_nr\n",
    "                             & (df_stamps_1633601['stamp_type'].str.contains('finished'))\n",
    "                            ]#['timestamp'] - _start_st\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "    ax.set_ylabel('worker')\n",
    "    # ax.set_xlim((0, _end_st - _start_st))\n",
    "\n",
    "    for ix in range(len(_sta)):\n",
    "        worker = _fin.iloc[ix][\"worker_id\"]\n",
    "        ax.barh(worker,\n",
    "                _fin.iloc[ix][\"timestamp\"] - _sta.iloc[ix][\"timestamp\"],\n",
    "                left=_sta.iloc[ix][\"timestamp\"] - _start_st,\n",
    "                color='red', linewidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633601\n",
    "_st_b0 = _st[_st['benchmark_number'] == 0]\n",
    "\n",
    "_jd = _st_b0['stamp_type'].str.contains('job done')\n",
    "\n",
    "_st_b0_jd_ask = _st_b0[_jd & _st_b0['stamp_type'].str.contains('asked')]\n",
    "_st_b0_jd_sta = _st_b0[_jd & _st_b0['stamp_type'].str.contains('started')]\n",
    "_st_b0_jd_fin = _st_b0[_jd & _st_b0['stamp_type'].str.contains('finished')]\n",
    "\n",
    "explicit_delay = 0\n",
    "implicit_delay = 0\n",
    "for g in _st_b0['gradient number'].unique():\n",
    "    for w in _st_b0['worker_id'].unique():\n",
    "        _sta_g = _st_b0_jd_sta[(_st_b0_jd_sta['gradient number'] == g) & (_st_b0_jd_sta['worker_id'] == w)]\n",
    "        _ask_g = _st_b0_jd_ask[(_st_b0_jd_ask['gradient number'] == g) & (_st_b0_jd_ask['worker_id'] == w)]\n",
    "        _fin_g = _st_b0_jd_fin[(_st_b0_jd_fin['gradient number'] == g) & (_st_b0_jd_fin['worker_id'] == w)]\n",
    "        explicit_delay += (_sta_g.reset_index()['timestamp'] - _ask_g.reset_index()['timestamp']).sum()/1.e9\n",
    "        implicit_delay += (_ask_g.iloc[1:].reset_index()['timestamp'] - _fin_g.iloc[:-1].reset_index()['timestamp']).sum()/1.e9\n",
    "\n",
    "explicit_delay, implicit_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633602\n",
    "_st_b0 = _st[_st['benchmark_number'] == 0]\n",
    "\n",
    "_jd = _st_b0['stamp_type'].str.contains('job done')\n",
    "\n",
    "_st_b0_jd_ask = _st_b0[_jd & _st_b0['stamp_type'].str.contains('asked')]\n",
    "_st_b0_jd_sta = _st_b0[_jd & _st_b0['stamp_type'].str.contains('started')]\n",
    "_st_b0_jd_fin = _st_b0[_jd & _st_b0['stamp_type'].str.contains('finished')]\n",
    "\n",
    "explicit_delay = 0\n",
    "implicit_delay = 0\n",
    "for g in _st_b0['gradient number'].unique():\n",
    "    for w in _st_b0['worker_id'].unique():\n",
    "        _sta_g = _st_b0_jd_sta[(_st_b0_jd_sta['gradient number'] == g) & (_st_b0_jd_sta['worker_id'] == w)]\n",
    "        _ask_g = _st_b0_jd_ask[(_st_b0_jd_ask['gradient number'] == g) & (_st_b0_jd_ask['worker_id'] == w)]\n",
    "        _fin_g = _st_b0_jd_fin[(_st_b0_jd_fin['gradient number'] == g) & (_st_b0_jd_fin['worker_id'] == w)]\n",
    "        explicit_delay += (_sta_g.reset_index()['timestamp'] - _ask_g.reset_index()['timestamp']).sum()/1.e9\n",
    "        implicit_delay += (_ask_g.iloc[1:].reset_index()['timestamp'] - _fin_g.iloc[:-1].reset_index()['timestamp']).sum()/1.e9\n",
    "\n",
    "explicit_delay, implicit_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633603\n",
    "_st_b0 = _st[_st['benchmark_number'] == 0]\n",
    "\n",
    "_jd = _st_b0['stamp_type'].str.contains('job done')\n",
    "\n",
    "_st_b0_jd_ask = _st_b0[_jd & _st_b0['stamp_type'].str.contains('asked')]\n",
    "_st_b0_jd_sta = _st_b0[_jd & _st_b0['stamp_type'].str.contains('started')]\n",
    "_st_b0_jd_fin = _st_b0[_jd & _st_b0['stamp_type'].str.contains('finished')]\n",
    "\n",
    "explicit_delay = 0\n",
    "implicit_delay = 0\n",
    "for g in _st_b0['gradient number'].unique():\n",
    "    for w in _st_b0['worker_id'].unique():\n",
    "        _sta_g = _st_b0_jd_sta[(_st_b0_jd_sta['gradient number'] == g) & (_st_b0_jd_sta['worker_id'] == w)]\n",
    "        _ask_g = _st_b0_jd_ask[(_st_b0_jd_ask['gradient number'] == g) & (_st_b0_jd_ask['worker_id'] == w)]\n",
    "        _fin_g = _st_b0_jd_fin[(_st_b0_jd_fin['gradient number'] == g) & (_st_b0_jd_fin['worker_id'] == w)]\n",
    "        explicit_delay += (_sta_g.reset_index()['timestamp'] - _ask_g.reset_index()['timestamp']).sum()/1.e9\n",
    "        implicit_delay += (_ask_g.iloc[1:].reset_index()['timestamp'] - _fin_g.iloc[:-1].reset_index()['timestamp']).sum()/1.e9\n",
    "\n",
    "explicit_delay, implicit_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all not that spectacular, except the 32 worker run, but that might also be due to actually having 34 processes to run (incl queue and master).\n",
    "\n",
    "Let's now sum up the delays caused by having to wait for one of the workers at the end + the corresponding effect at the beginning (the latter effect is a lot smaller, the workers start more synchronously than they end).\n",
    "\n",
    "There are several ways you can measure the impact of these worker idle times.\n",
    "\n",
    "- You can sum the total time that all workers wait, which gives you a sum total of all the potential calculation time you could have used while the slowest worker was still working.\n",
    "- But this doesn't tell you how fast it could have actually been if load balancing were perfect. To get the time you could have saved if you had perfect load balancing, divide the above number by the number of workers. **This is the actual time lost due to imperfect load balancing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633601\n",
    "for b in _st['benchmark_number'].dropna().unique():\n",
    "    _st_b0 = _st[_st['benchmark_number'] == b]\n",
    "\n",
    "    _jd = _st_b0['stamp_type'].str.contains('job done')\n",
    "\n",
    "    _st_b0_jd_ask = _st_b0[_jd & _st_b0['stamp_type'].str.contains('asked')]\n",
    "    _st_b0_jd_sta = _st_b0[_jd & _st_b0['stamp_type'].str.contains('started')]\n",
    "    _st_b0_jd_fin = _st_b0[_jd & _st_b0['stamp_type'].str.contains('finished')]\n",
    "\n",
    "    start_delay = 0\n",
    "    finish_delay = 0\n",
    "    for g in _st_b0['gradient number'].dropna().unique():\n",
    "        min_start = _st_b0_jd_sta[(_st_b0_jd_sta['gradient number'] == g)]['timestamp'].min()\n",
    "        max_finish = _st_b0_jd_fin[(_st_b0_jd_fin['gradient number'] == g)]['timestamp'].max()\n",
    "        for w in _st_b0['worker_id'].dropna().unique():\n",
    "            start_delay += (_st_b0_jd_sta[(_st_b0_jd_sta['gradient number'] == g)\n",
    "                                          & (_st_b0_jd_sta['worker_id'] == w)\n",
    "                                         ]['timestamp'].min() - min_start) / 1.e9\n",
    "            finish_delay += (max_finish - _st_b0_jd_fin[(_st_b0_jd_fin['gradient number'] == g)\n",
    "                                           & (_st_b0_jd_fin['worker_id'] == w)\n",
    "                                          ]['timestamp'].max()) / 1.e9\n",
    "    print('benchmark', b, 'with', len(_st_b0['worker_id'].dropna().unique()), 'workers')\n",
    "    print('start delay: ', start_delay)\n",
    "    print('finish delay:', finish_delay)\n",
    "    print('total delay: ', start_delay + finish_delay)\n",
    "    print('time lost due to imperfect load balancing: ', (start_delay + finish_delay) / len(_st_b0['worker_id'].dropna().unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that is negligible.\n",
    "\n",
    "# CPU time\n",
    "\n",
    "Suggestion by Wouter: compare the partial derivative wall time with the CPU time. This way we can find out whether something is really being delayed in the calculation itself, or whether we should look elsewhere.\n",
    "\n",
    "Did another acat19.cpp run including CPU timing, now with 8 workers and without the `mu = 1.5` line.\n",
    "\n",
    "Have to again modify the functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_split_timing_run(timing_grouped_lines_list, line_search_lines, update_queue, update_worker, terminate_line):\n",
    "    data = {'time [s]': [], 'timing_type': [], 'worker_id': [], 'task': []}\n",
    "\n",
    "    for gradient_call_timings in timing_grouped_lines_list:\n",
    "        words = gradient_call_timings['gradient_total'].split()\n",
    "\n",
    "        data['time [s]'].append(float(words[4][:-2]))\n",
    "        data['timing_type'].append('update state')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "        data['time [s]'].append(float(words[11][:-1]))\n",
    "        data['timing_type'].append('gradient work')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "        \n",
    "        for partial_derivative_line in gradient_call_timings['partial_derivatives']:\n",
    "            words = partial_derivative_line.split()\n",
    "            try:\n",
    "                data['worker_id'].append(words[4][:-1])\n",
    "                data['task'].append(words[6][:-1])\n",
    "                data['time [s]'].append(float(words[10][:-1]))\n",
    "                data['timing_type'].append('partial derivative')\n",
    "                \n",
    "                if len(words) > 13:\n",
    "                    data['worker_id'].append(words[4][:-1])\n",
    "                    data['task'].append(words[6][:-1])\n",
    "                    data['time [s]'].append(float(words[13][:-1]))\n",
    "                    data['timing_type'].append('partial derivative CPU time')\n",
    "            except ValueError as e:\n",
    "                print(words)\n",
    "                raise e\n",
    "\n",
    "    for line_search_line in line_search_lines:\n",
    "        words = line_search_line.split()\n",
    "        data['time [s]'].append(float(words[1][:-1]))\n",
    "        data['timing_type'].append('line search')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "    for line in update_queue:\n",
    "        words = line.split()\n",
    "        data['time [s]'].append(float(words[6][:-1]))\n",
    "        data['timing_type'].append('update_real on queue')\n",
    "        data['worker_id'].append(None)\n",
    "        data['task'].append(None)\n",
    "\n",
    "    for line in update_worker:\n",
    "        words = line.split()\n",
    "        data['time [s]'].append(float(words[7][:-1]))\n",
    "        data['timing_type'].append('update_real on worker')\n",
    "        data['worker_id'].append(int(words[6][:-1]))\n",
    "        data['task'].append(None)\n",
    "\n",
    "    words = terminate_line.split()\n",
    "    data['time [s]'].append(float(words[4][:-1]))\n",
    "    data['timing_type'].append('terminate')\n",
    "    data['worker_id'].append(None)\n",
    "    data['task'].append(None)\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_acat19_1552664372 = '/Users/pbos/projects/apcocsm/code/acat19_1552664372.out'\n",
    "\n",
    "df_split_timings_acat19_1552664372, df_stamps_acat19_1552664372 = build_comb_df_split_timing_info(fn_acat19_1552664372,\n",
    "                                                                                 extract_fcn=load_acat19_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_timings_acat19_1552664372[df_split_timings_acat19_1552664372['timing_type'] == 'partial derivative']['time [s]'].sum(),\\\n",
    "df_split_timings_acat19_1552664372[df_split_timings_acat19_1552664372['timing_type'] == 'partial derivative CPU time']['time [s]'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are almost completely equal, which is good news: we don't have to look anywhere else, like in scheduling or something. There is really something strange going on in the partial derivatives themselves that makes their total time grow.\n",
    "\n",
    "Note that these numbers in absolute sense are not comparable to the earlier big fit runs, because those were on the schudeled nodes, and this was run on the stoomboot headnode! Indeed, that seems to have 2400 MHz cores, while google benchmark reports 1996 MHz (weird, but ok) for the compute nodes.\n",
    "\n",
    "# What about the big REST block?\n",
    "\n",
    "We may be onto the culprit. Put timing around the SetInitialGradient function...\n",
    "\n",
    "Shit, nope. See the `acat19_1552671011.out` file. Three times ~0.0003 seconds. That's nothing. In hindsight, it makes sense: SetInitialGradient never calls the function (the likelihood), so it's not at all expensive.\n",
    "\n",
    "Wouter suspected that somehow **Simplex** gets run somewhere before Migrad, but I haven't been able to find it anywhere... at least not starting from `RooMinimizer::migrad`. Maybe it is somewhere else, perhaps somewhere in the Migrad/Minuit setup.\n",
    "\n",
    "We could time the Migrad Seed creation to be sure...\n",
    "\n",
    "But maybe it's easier to just throw in some timestamps, then at least we can pin down where in migrad things are happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "In the meantime, why not take a look at those...\n",
    "# update_state timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_timing_lines(bm_iteration_lines):\n",
    "    \"\"\"\n",
    "    Group lines (from one benchmark iteration) by gradient call,\n",
    "    specifying:\n",
    "    - Update time on master\n",
    "    - update_real times on queue and workers\n",
    "    - Gradient work time\n",
    "    - For all partial derivatives a sublist of all lines\n",
    "    - After each gradient block, there may be line_search times,\n",
    "      these are also included as a sublist\n",
    "    Finally, the terminate time for the entire bm_iteration is also\n",
    "    returned (last line in the list).\n",
    "    \n",
    "    In each gradient, also timestamps are printed. These are not\n",
    "    further subdivided in this function, but are output as part of\n",
    "    the `gradient_calls` list for further processing elsewhere.\n",
    "    \"\"\"\n",
    "    gradient_calls = []\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    line_search_lines = []\n",
    "    update_queue = []\n",
    "    update_worker = []\n",
    "    # flag to check whether we are still in the same gradient call\n",
    "    in_block = False\n",
    "    for ix, line in enumerate(bm_iteration_lines[:-1]):  # -1: leave out terminate line\n",
    "        if not in_block and (line[:9] == 'worker_id' or line[:24] == '[#0] DEBUG: -- worker_id'):\n",
    "            start_indices.append(ix)\n",
    "            in_block = True\n",
    "        elif 'update_state' in line:\n",
    "            end_indices.append(ix)\n",
    "            in_block = False\n",
    "        # the rest has nothing to do with the gradient call block, so we don't touch in_block there:\n",
    "        elif 'line_search' in line:\n",
    "            line_search_lines.append(line)\n",
    "        elif 'update_real on queue' in line:\n",
    "            update_queue.append(line)\n",
    "        elif 'update_real on worker' in line:\n",
    "            update_worker.append(line)\n",
    "        elif 'start migrad' in line:\n",
    "            start_migrad_line = line\n",
    "        elif 'end migrad' in line:\n",
    "            end_migrad_line = line\n",
    "    \n",
    "    if len(start_indices) != len(end_indices):\n",
    "        raise Exception(f\"Number of start and end indices unequal (resp. {len(start_indices)} and {len(end_indices)})!\")\n",
    "        \n",
    "    for ix in range(len(start_indices)):\n",
    "        partial_derivatives, timestamps = separate_partderi_job_time_lines(bm_iteration_lines[start_indices[ix]:end_indices[ix]])\n",
    "        gradient_calls.append({\n",
    "            'gradient_total': bm_iteration_lines[end_indices[ix]],\n",
    "            'partial_derivatives': partial_derivatives,\n",
    "            'timestamps': timestamps\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        terminate_line = bm_iteration_lines[-1]\n",
    "    except IndexError:\n",
    "        terminate_line = None\n",
    "        \n",
    "    special_lines = dict(terminate_line=terminate_line,\n",
    "                         start_migrad_line=start_migrad_line,\n",
    "                         end_migrad_line=end_migrad_line)\n",
    "\n",
    "    return gradient_calls, line_search_lines, update_queue, update_worker, special_lines\n",
    "\n",
    "\n",
    "def build_df_stamps(grouped_lines, special_lines, update_queue, update_worker):\n",
    "    data = {'timestamp': [], 'stamp_type': [], 'worker_id': []}\n",
    "\n",
    "    words = special_lines[\"start_migrad_line\"].split()\n",
    "    shift = 3 if '[#' in words[0] else 0\n",
    "    data['timestamp'].append(int(words[3 + shift]))\n",
    "    data['stamp_type'].append('start migrad')\n",
    "    data['worker_id'].append(None)\n",
    "\n",
    "    if len(words) > 10:\n",
    "        NumCPU = int(words[10])\n",
    "    else:\n",
    "        NumCPU = 0\n",
    "\n",
    "    words = special_lines[\"end_migrad_line\"].split()\n",
    "    shift = 3 if '[#' in words[0] else 0\n",
    "    data['timestamp'].append(int(words[3 + shift]))\n",
    "    data['stamp_type'].append('end migrad')\n",
    "    data['worker_id'].append(None)\n",
    "\n",
    "    for gradient_group in grouped_lines:\n",
    "        for line in gradient_group['timestamps']:\n",
    "            if 'no work' in line:\n",
    "                words = line.split()\n",
    "                shift = 3 if '[#' in words[0] else 0\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[6 + shift]))\n",
    "                data['stamp_type'].append('no job - asked')\n",
    "\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[11 + shift]))\n",
    "                data['stamp_type'].append('no job - denied')\n",
    "            elif 'job done' in line:\n",
    "                words = line.split()\n",
    "                shift = 3 if '[#' in words[0] else 0\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[6 + shift][:-1]))\n",
    "                data['stamp_type'].append('job done - asked')\n",
    "\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[9 + shift]))\n",
    "                data['stamp_type'].append('job done - started')\n",
    "\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[13 + shift]))\n",
    "                data['stamp_type'].append('job done - finished')\n",
    "            else:\n",
    "                raise Exception(\"got a weird line:\\n\" + line)\n",
    "\n",
    "    for line in update_queue:\n",
    "        words = line.split()\n",
    "        shift = 3 if '[#' in words[0] else 0\n",
    "        data['worker_id'].append(None)\n",
    "        data['timestamp'].append(int(words[5 + shift]))\n",
    "        data['stamp_type'].append('update_real queue start')\n",
    "\n",
    "        data['worker_id'].append(None)\n",
    "        data['timestamp'].append(int(words[7 + shift][:-3]))\n",
    "        data['stamp_type'].append('update_real queue end')\n",
    "\n",
    "    for line in update_worker:\n",
    "        words = line.split()\n",
    "        shift = 3 if '[#' in words[0] else 0\n",
    "        data['worker_id'].append(int(words[3 + shift][:-1]))\n",
    "        data['timestamp'].append(int(words[6 + shift]))\n",
    "        data['stamp_type'].append('update_real worker start')\n",
    "\n",
    "        data['worker_id'].append(int(words[3 + shift][:-1]))\n",
    "        data['timestamp'].append(int(words[8 + shift][:-3]))\n",
    "        data['stamp_type'].append('update_real worker end')\n",
    "\n",
    "    return pd.DataFrame(data), NumCPU\n",
    "\n",
    "\n",
    "def build_dflist_split_timing_info(fn, extract_fcn=extract_split_timing_info):\n",
    "    bm_iterations = extract_fcn(fn)\n",
    "\n",
    "    dflist = []\n",
    "    dflist_stamps = []\n",
    "    NumCPU_list = []\n",
    "    for bm in bm_iterations:\n",
    "        grouped_lines, line_search_lines, update_queue, update_worker, special_lines = group_timing_lines(bm)\n",
    "        if special_lines[\"terminate_line\"] is not None:\n",
    "            dflist.append(build_df_split_timing_run(grouped_lines, line_search_lines, update_queue, update_worker, special_lines[\"terminate_line\"]))\n",
    "\n",
    "            df_stamps, NumCPU = build_df_stamps(grouped_lines, special_lines, update_queue, update_worker)\n",
    "            dflist_stamps.append(df_stamps)\n",
    "            NumCPU_list.append(NumCPU)\n",
    "\n",
    "    return dflist, dflist_stamps, NumCPU_list\n",
    "\n",
    "\n",
    "def build_comb_df_split_timing_info(fn, extract_fcn=extract_split_timing_info):\n",
    "    dflist, dflist_stamps, NumCPU_list = build_dflist_split_timing_info(fn, extract_fcn=extract_fcn)\n",
    "\n",
    "    for ix, df in enumerate(dflist):\n",
    "        df_pardiff = df[df[\"timing_type\"] == \"partial derivative\"]\n",
    "        N_tasks = len(df_pardiff[\"task\"].unique())\n",
    "        N_gradients = len(df_pardiff) // N_tasks\n",
    "        gradient_indices = np.hstack(i * np.ones(N_tasks, dtype='int') for i in range(N_gradients))\n",
    "\n",
    "        df[\"gradient number\"] = pd.Series(dtype='Int64')\n",
    "        df.loc[df[\"timing_type\"] == \"partial derivative\", \"gradient number\"] = gradient_indices\n",
    "        \n",
    "        dflist_stamps[ix][\"gradient number\"] = pd.Series(dtype='Int64')\n",
    "        dflist_stamps[ix].loc[dflist_stamps[ix][\"stamp_type\"] == \"job done - asked\", \"gradient number\"] = gradient_indices\n",
    "        dflist_stamps[ix].loc[dflist_stamps[ix][\"stamp_type\"] == \"job done - started\", \"gradient number\"] = gradient_indices\n",
    "        dflist_stamps[ix].loc[dflist_stamps[ix][\"stamp_type\"] == \"job done - finished\", \"gradient number\"] = gradient_indices\n",
    "\n",
    "        df[\"benchmark_number\"] = ix\n",
    "        dflist_stamps[ix][\"benchmark_number\"] = ix\n",
    "\n",
    "    # assuming the stamps are ordered properly, which I'm pretty sure is correct,\n",
    "    # we can do ffill:\n",
    "    df_stamps = pd.concat(dflist_stamps)\n",
    "    df_stamps.loc[~df_stamps['stamp_type'].str.contains('migrad'), 'gradient number'] = df_stamps.loc[~df_stamps['stamp_type'].str.contains('migrad'), 'gradient number'].fillna(method='ffill')\n",
    "        \n",
    "    return pd.concat(dflist), df_stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, df_stamps_1633603 = build_comb_df_split_timing_info('../rootbench/1633603.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_1633602 = build_comb_df_split_timing_info('../rootbench/1633602.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_1633601 = build_comb_df_split_timing_info('../rootbench/1633601.burrell.nikhef.nl.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633601\n",
    "_st['stamp_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update_real queue timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633601\n",
    "\n",
    "_bench_nr = (_st['benchmark_number'] == 0)\n",
    "\n",
    "_start_st = _st[_bench_nr\n",
    "                              & (_st['stamp_type'] == 'start migrad')\n",
    "                             ]['timestamp'].iloc[0]\n",
    "_end_st = _st[_bench_nr\n",
    "                              & (_st['stamp_type'] == 'end migrad')\n",
    "                             ]['timestamp'].iloc[0]\n",
    "\n",
    "_sta = _st[_bench_nr\n",
    "                         & (_st['stamp_type'] == 'update_real queue start')\n",
    "                        ]#['timestamp'] - _start_st\n",
    "_fin = _st[_bench_nr\n",
    "                         & (_st['stamp_type'] == 'update_real queue end')\n",
    "                        ]#['timestamp'] - _start_st\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "# ax.set_ylabel('worker')\n",
    "ax.set_xlim((0, _end_st - _start_st))\n",
    "\n",
    "# worker = _fin.iloc[ix][\"worker_id\"]\n",
    "ax.plot(_sta[\"timestamp\"] - _start_st,\n",
    "        np.zeros_like(_sta[\"timestamp\"]),\n",
    "        color='blue', marker='|', linestyle='', alpha=0.5)\n",
    "ax.plot(_fin[\"timestamp\"] - _start_st,\n",
    "        np.zeros_like(_sta[\"timestamp\"]),\n",
    "        color='red', marker='|', linestyle='', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, this works a lot better than the bar plots, at least we can show everything like this.\n",
    "\n",
    "Let's also try for the job stamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_st = df_stamps_1633601\n",
    "\n",
    "_bench_nr = (_st['benchmark_number'] == 0)\n",
    "\n",
    "_start_st = _st[_bench_nr\n",
    "                              & (_st['stamp_type'] == 'start migrad')\n",
    "                             ]['timestamp'].iloc[0]\n",
    "_end_st = _st[_bench_nr\n",
    "                              & (_st['stamp_type'] == 'end migrad')\n",
    "                             ]['timestamp'].iloc[0]\n",
    "\n",
    "_sta = _st[_bench_nr\n",
    "                         & (_st['stamp_type'].str.contains('started'))\n",
    "                        ]#['timestamp'] - _start_st\n",
    "_fin = _st[_bench_nr\n",
    "                         & (_st['stamp_type'].str.contains('finished'))\n",
    "                        ]#['timestamp'] - _start_st\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "ax.set_ylabel('worker')\n",
    "ax.set_xlim((0, _end_st - _start_st))\n",
    "\n",
    "ax.plot(_sta[\"timestamp\"] - _start_st, _sta[\"worker_id\"],\n",
    "        color='green', marker='|', linestyle='', alpha=0.5)\n",
    "ax.plot(_fin[\"timestamp\"] - _start_st, _fin[\"worker_id\"],\n",
    "        color='blue', marker='|', linestyle='', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Let's combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timestamps(_st, bench_nr=0, figsize=(20, 5)):\n",
    "    vert_offset = 0.1\n",
    "\n",
    "    _bench_nr = (_st['benchmark_number'] == bench_nr)\n",
    "\n",
    "    _start_st = _st[_bench_nr & (_st['stamp_type'] == 'start migrad')]['timestamp'].iloc[0]\n",
    "    _end_st = _st[_bench_nr & (_st['stamp_type'] == 'end migrad')]['timestamp'].iloc[0]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_ylabel('worker')\n",
    "    ax.set_xlim((0, _end_st - _start_st))\n",
    "\n",
    "    # update @ queue\n",
    "    _sta = _st[_bench_nr & (_st['stamp_type'].str.contains('update_real queue start'))]\n",
    "    _fin = _st[_bench_nr & (_st['stamp_type'].str.contains('update_real queue end'))]\n",
    "\n",
    "    ax.plot(_sta[\"timestamp\"] - _start_st, np.zeros_like(_sta[\"timestamp\"]) - 1 - vert_offset,\n",
    "            color='red', marker='|', linestyle='', alpha=0.5)\n",
    "    ax.plot(_fin[\"timestamp\"] - _start_st, np.zeros_like(_fin[\"timestamp\"]) - 1 + vert_offset,\n",
    "            color='red', marker='|', linestyle='', alpha=0.5)\n",
    "\n",
    "    # update @ queue\n",
    "    _sta = _st[_bench_nr & (_st['stamp_type'].str.contains('update_real worker start'))]\n",
    "    _fin = _st[_bench_nr & (_st['stamp_type'].str.contains('update_real worker end'))]\n",
    "\n",
    "    ax.plot(_sta[\"timestamp\"] - _start_st, _sta[\"worker_id\"] - vert_offset,\n",
    "            color='red', marker='|', linestyle='', alpha=0.5)\n",
    "    ax.plot(_fin[\"timestamp\"] - _start_st, _fin[\"worker_id\"] + vert_offset,\n",
    "            color='red', marker='|', linestyle='', alpha=0.5)\n",
    "\n",
    "    # jobs\n",
    "    _sta = _st[_bench_nr & (_st['stamp_type'].str.contains('started'))]\n",
    "    _fin = _st[_bench_nr & (_st['stamp_type'].str.contains('finished'))]\n",
    "\n",
    "    ax.plot(_sta[\"timestamp\"] - _start_st, _sta[\"worker_id\"] - vert_offset,\n",
    "            color='blue', marker='|', linestyle='', alpha=0.8)\n",
    "    ax.plot(_fin[\"timestamp\"] - _start_st, _fin[\"worker_id\"] + vert_offset,\n",
    "            color='blue', marker='|', linestyle='', alpha=0.8)\n",
    "\n",
    "    # out of jobs!\n",
    "    _sta = _st[_bench_nr & (_st['stamp_type'].str.contains('no job - asked'))]\n",
    "    _fin = _st[_bench_nr & (_st['stamp_type'].str.contains('no job - denied'))]\n",
    "\n",
    "    ax.plot(_sta[\"timestamp\"] - _start_st, _sta[\"worker_id\"] - vert_offset,\n",
    "            color='grey', marker='|', linestyle='', alpha=0.3)\n",
    "    ax.plot(_fin[\"timestamp\"] - _start_st, _fin[\"worker_id\"] + vert_offset,\n",
    "            color='grey', marker='|', linestyle='', alpha=0.3)\n",
    "    \n",
    "    # migrad timestamps\n",
    "    migrad = _st[_bench_nr & (_st['stamp_type'].str.contains('migrad timestamp'))]\n",
    "    \n",
    "    ax.plot(migrad[\"timestamp\"] - _start_st, np.zeros_like(migrad[\"timestamp\"]) - 2,\n",
    "            color='black', marker='|', linestyle='')\n",
    "\n",
    "    # setfcn timestamps\n",
    "    _sta = _st[_bench_nr & (_st['stamp_type'].str.contains('start SetFCN'))]\n",
    "    _fin = _st[_bench_nr & (_st['stamp_type'].str.contains('end SetFCN'))]\n",
    "    \n",
    "    ax.plot(_sta[\"timestamp\"] - _start_st, np.zeros_like(_sta[\"timestamp\"]) - 2 - vert_offset,\n",
    "            color='violet', marker='|', linestyle='', alpha=0.9)\n",
    "    ax.plot(_fin[\"timestamp\"] - _start_st, np.zeros_like(_fin[\"timestamp\"]) - 2 + vert_offset,\n",
    "            color='violet', marker='|', linestyle='', alpha=0.9)\n",
    "    \n",
    "    # GradFcnSynchronize timestamps\n",
    "    GradFcnSynchronize = _st[_bench_nr & (_st['stamp_type'].str.contains('GradFcnSynchronize timestamp'))]\n",
    "    \n",
    "    ax.plot(GradFcnSynchronize[\"timestamp\"] - _start_st, np.zeros_like(GradFcnSynchronize[\"timestamp\"]) - 2,\n",
    "            color='tab:green', marker='|', linestyle='', alpha=0.9)\n",
    "    \n",
    "    # setup_differentiate timestamps\n",
    "    _this = _st[_bench_nr & (_st['stamp_type'].str.contains('setup_differentiate timestamps'))]\n",
    "    \n",
    "    ax.plot(_this[\"timestamp\"] - _start_st, _this['worker_id'],\n",
    "            color='tab:orange', marker='|', linestyle='', alpha=0.9)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timestamps(df_stamps_1633601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, interesting observation: there is actually a long job at the start of every gradient calculation iteration **on every worker**. This does hint quite strongly at either recalculation of the cache or perhaps of something to do with the sync_parameters stuff... We should measure this first task of each gradient in more detail, because this might very well explain (a big part of) the non-scaling of the gradient!\n",
    "\n",
    "After the blue but before the red parts the line search probably takes place. Don't have timestamps for them, but they take between about 0.7 and 0.9 seconds, that fits nicely with those gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does ZeroMQ performance explain the update state durations?\n",
    "\n",
    "Did some measurements of the ZeroMQ performance using the provided performance measurement tools. At least on my Macbook, the red parts should take about 0.1s for 8 workers, so this ~1 second is a bit mysterious. It could be that the performance is different on Stoomboot though...\n",
    "\n",
    "No, actually it turns out the performance of the Stoomboot headnode (where these runs were done) is a bit higher than my Macbook! So it should be even more than a factor 10 faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 cores\n",
    "Is it the case that by coincidence the first 8 tasks are longer than the rest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timestamps(df_stamps_1633602)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "Discussed with Wouter, we can make two quick tests to see what's going on here:\n",
    "\n",
    "- For the longer initial tasks: let's measure this on an N-dim Gaussian (128 or something) so that we can exclude computational causes, because all components should then take exactly the same amount of time.\n",
    "\n",
    "- For the communication overhead, as a quick test for the ZMQ throughput: send twice the amount of data to see if the time indeed increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-D gaussian, run on stoomboot head node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, df_stamps_acat_ND_gauss_1553700233 = build_comb_df_split_timing_info('../acat19_ND_gauss_1553700233.out', extract_fcn=load_acat19_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, that one doesn't have migrad start and end timestamps... fixed that in the next one, plus new loading function that doesn't add dummy lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acat19_out_v2(fn):\n",
    "    \"\"\"\n",
    "    Just single migrad runs, so no need for further splitting by run.\n",
    "    Does add dummy terminate line, because the other functions expect\n",
    "    this. No more dummy start and end migrad lines.\n",
    "    \"\"\"\n",
    "    with open(fn, 'r') as fh:\n",
    "        lines = fh.read().splitlines()\n",
    "    print(lines[-1])\n",
    "    lines.append('[#0] DEBUG: -- terminate: 0.0s')    \n",
    "    return [lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_ND_gauss_1553701454 = build_comb_df_split_timing_info('../acat19_ND_gauss_1553701454.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_ND_gauss_1553701454)\n",
    "ax.set_xlim(0, 0.2e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try without randomized parameters, hopefully the run will be a bit shorter then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_ND_gauss_1553754554 = build_comb_df_split_timing_info('../acat19_ND_gauss_1553754554.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_ND_gauss_1553754554)\n",
    "ax.set_xlim(0, 0.2e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, still pretty long. Same picture though, indeed also here there's a slight increase in the runtime for the first tasks of the run.\n",
    "\n",
    "Could this scale with the number of parameters? The above runs were with 128 parameters, Let's try a run with 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_ND_gauss_1553755013 = build_comb_df_split_timing_info('../acat19_ND_gauss_1553755013.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_ND_gauss_1553755013)\n",
    "# ax.set_xlim(0, 0.2e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, bit weird, only one iteration in this case... maybe reactivate parameter randomization here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, one other thing, I added the \"migrad timestamps\", should read those in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_timing_lines(bm_iteration_lines):\n",
    "    \"\"\"\n",
    "    Group lines (from one benchmark iteration) by gradient call,\n",
    "    specifying:\n",
    "    - Update time on master\n",
    "    - update_real times on queue and workers\n",
    "    - Gradient work time\n",
    "    - For all partial derivatives a sublist of all lines\n",
    "    - After each gradient block, there may be line_search times,\n",
    "      these are also included as a sublist\n",
    "    Finally, the terminate time for the entire bm_iteration is also\n",
    "    returned (last line in the list).\n",
    "    \n",
    "    In each gradient, also timestamps are printed. These are not\n",
    "    further subdivided in this function, but are output as part of\n",
    "    the `gradient_calls` list for further processing elsewhere.\n",
    "    \"\"\"\n",
    "    gradient_calls = []\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    line_search_lines = []\n",
    "    update_queue = []\n",
    "    update_worker = []\n",
    "    setfcn_timestamps = []\n",
    "    setup_differentiate_timestamps = []\n",
    "    \n",
    "    # to prevent UnboundLocalErrors in older data files:\n",
    "    GradFcnSynchronize_timestamps = None\n",
    "\n",
    "    # flag to check whether we are still in the same gradient call\n",
    "    in_block = False\n",
    "    for ix, line in enumerate(bm_iteration_lines[:-1]):  # -1: leave out terminate line\n",
    "        if not in_block and (line[:9] == 'worker_id' or line[:24] == '[#0] DEBUG: -- worker_id'):\n",
    "            start_indices.append(ix)\n",
    "            in_block = True\n",
    "        elif 'update_state' in line:\n",
    "            end_indices.append(ix)\n",
    "            in_block = False\n",
    "        # the rest has nothing to do with the gradient call block, so we don't touch in_block there:\n",
    "        elif 'line_search' in line:\n",
    "            line_search_lines.append(line)\n",
    "        elif 'update_real on queue' in line:\n",
    "            update_queue.append(line)\n",
    "        elif 'update_real on worker' in line:\n",
    "            update_worker.append(line)\n",
    "        elif 'start migrad' in line:\n",
    "            start_migrad_line = line\n",
    "        elif 'end migrad' in line:\n",
    "            end_migrad_line = line\n",
    "        elif 'migrad timestamps' in line:\n",
    "            migrad_timestamps = line\n",
    "        elif 'Fitter::SetFCN timestamps' in line:\n",
    "            setfcn_timestamps.append(line)\n",
    "        elif 'RooGradientFunction::synchronize_parameter_settings timestamps' in line:\n",
    "            GradFcnSynchronize_timestamps = line\n",
    "        elif 'NumericalDerivatorMinuit2::setup_differentiate' in line:\n",
    "            setup_differentiate_timestamps.append(line)\n",
    "    \n",
    "    if len(start_indices) != len(end_indices):\n",
    "        raise Exception(f\"Number of start and end indices unequal (resp. {len(start_indices)} and {len(end_indices)})!\")\n",
    "        \n",
    "    for ix in range(len(start_indices)):\n",
    "        partial_derivatives, timestamps = separate_partderi_job_time_lines(bm_iteration_lines[start_indices[ix]:end_indices[ix]])\n",
    "        gradient_calls.append({\n",
    "            'gradient_total': bm_iteration_lines[end_indices[ix]],\n",
    "            'partial_derivatives': partial_derivatives,\n",
    "            'timestamps': timestamps\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        terminate_line = bm_iteration_lines[-1]\n",
    "    except IndexError:\n",
    "        terminate_line = None\n",
    "        \n",
    "    special_lines = dict(terminate_line=terminate_line,\n",
    "                         start_migrad_line=start_migrad_line,\n",
    "                         end_migrad_line=end_migrad_line,\n",
    "                         migrad_timestamps=migrad_timestamps,\n",
    "                         setfcn_timestamps=setfcn_timestamps,\n",
    "                         GradFcnSynchronize_timestamps=GradFcnSynchronize_timestamps,\n",
    "                         setup_differentiate_timestamps=setup_differentiate_timestamps)\n",
    "\n",
    "    return gradient_calls, line_search_lines, update_queue, update_worker, special_lines\n",
    "\n",
    "\n",
    "def build_df_stamps(grouped_lines, special_lines, update_queue, update_worker):\n",
    "    data = {'timestamp': [], 'stamp_type': [], 'worker_id': []}\n",
    "\n",
    "    words = special_lines[\"start_migrad_line\"].split()\n",
    "    shift = 3 if '[#' in words[0] else 0\n",
    "    data['timestamp'].append(int(words[3 + shift]))\n",
    "    data['stamp_type'].append('start migrad')\n",
    "    data['worker_id'].append(None)\n",
    "\n",
    "    if len(words) > 10:\n",
    "        NumCPU = int(words[10])\n",
    "    else:\n",
    "        NumCPU = 0\n",
    "\n",
    "    words = special_lines[\"end_migrad_line\"].split()\n",
    "    shift = 3 if '[#' in words[0] else 0\n",
    "    data['timestamp'].append(int(words[3 + shift]))\n",
    "    data['stamp_type'].append('end migrad')\n",
    "    data['worker_id'].append(None)\n",
    "\n",
    "    words = special_lines[\"migrad_timestamps\"].split()\n",
    "    shift = 3 if '[#' in words[0] else 0\n",
    "    for word in words[shift + 2:]:\n",
    "        data['timestamp'].append(int(word))\n",
    "        data['stamp_type'].append('migrad timestamp')\n",
    "        data['worker_id'].append(None)\n",
    "    \n",
    "    if special_lines[\"GradFcnSynchronize_timestamps\"] is not None:\n",
    "        words = special_lines[\"GradFcnSynchronize_timestamps\"].split()\n",
    "        shift = 3 if '[#' in words[0] else 0\n",
    "        for word in words[shift + 2:]:\n",
    "            data['timestamp'].append(int(word))\n",
    "            data['stamp_type'].append('GradFcnSynchronize timestamp')\n",
    "            data['worker_id'].append(None)\n",
    "\n",
    "    for line in special_lines[\"setfcn_timestamps\"]:\n",
    "        words = line.split()\n",
    "        shift = 3 if '[#' in words[0] else 0\n",
    "        \n",
    "        data['timestamp'].append(int(words[shift + 2]))\n",
    "        data['stamp_type'].append('start SetFCN')\n",
    "        data['worker_id'].append(None)\n",
    "\n",
    "        data['timestamp'].append(int(words[shift + 3]))\n",
    "        data['stamp_type'].append('end SetFCN')\n",
    "        data['worker_id'].append(None)\n",
    "\n",
    "\n",
    "    for line in special_lines[\"setup_differentiate_timestamps\"]:\n",
    "        words = line.split()\n",
    "        shift = 3 if '[#' in words[0] else 0\n",
    "        worker = int(words[shift + 3][:-1])\n",
    "        for word in words[shift + 5:]:\n",
    "            data['timestamp'].append(int(word))\n",
    "            data['stamp_type'].append('setup_differentiate timestamps')\n",
    "            data['worker_id'].append(worker)\n",
    "\n",
    "\n",
    "    for gradient_group in grouped_lines:\n",
    "        for line in gradient_group['timestamps']:\n",
    "            if 'no work' in line:\n",
    "                words = line.split()\n",
    "                shift = 3 if '[#' in words[0] else 0\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[6 + shift]))\n",
    "                data['stamp_type'].append('no job - asked')\n",
    "\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[11 + shift]))\n",
    "                data['stamp_type'].append('no job - denied')\n",
    "            elif 'job done' in line:\n",
    "                words = line.split()\n",
    "                shift = 3 if '[#' in words[0] else 0\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[6 + shift][:-1]))\n",
    "                data['stamp_type'].append('job done - asked')\n",
    "\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[9 + shift]))\n",
    "                data['stamp_type'].append('job done - started')\n",
    "\n",
    "                data['worker_id'].append(int(words[3 + shift]))\n",
    "                data['timestamp'].append(int(words[13 + shift]))\n",
    "                data['stamp_type'].append('job done - finished')\n",
    "            elif 'NumericalDerivatorMinuit2::setup_differentiate' in line:\n",
    "                pass\n",
    "            elif 'fVal on worker' in line or 'fVal after line search' in line:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"got a weird line:\\n\" + line)\n",
    "\n",
    "    for line in update_queue:\n",
    "        words = line.split()\n",
    "        shift = 3 if '[#' in words[0] else 0\n",
    "        data['worker_id'].append(None)\n",
    "        data['timestamp'].append(int(words[5 + shift]))\n",
    "        data['stamp_type'].append('update_real queue start')\n",
    "\n",
    "        data['worker_id'].append(None)\n",
    "        data['timestamp'].append(int(words[7 + shift][:-3]))\n",
    "        data['stamp_type'].append('update_real queue end')\n",
    "\n",
    "    for line in update_worker:\n",
    "        words = line.split()\n",
    "        shift = 3 if '[#' in words[0] else 0\n",
    "        data['worker_id'].append(int(words[3 + shift][:-1]))\n",
    "        data['timestamp'].append(int(words[6 + shift]))\n",
    "        data['stamp_type'].append('update_real worker start')\n",
    "\n",
    "        data['worker_id'].append(int(words[3 + shift][:-1]))\n",
    "        data['timestamp'].append(int(words[8 + shift][:-3]))\n",
    "        data['stamp_type'].append('update_real worker end')\n",
    "\n",
    "    return pd.DataFrame(data), NumCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_ND_gauss_1553754554 = build_comb_df_split_timing_info('../acat19_ND_gauss_1553754554.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_ND_gauss_1553754554)\n",
    "ax.set_xlim(-1e9, 3e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_st = df_stamps_acat_ND_gauss_1553754554[df_stamps_acat_ND_gauss_1553754554['stamp_type'] == \"start migrad\"]\n",
    "df_stamps_acat_ND_gauss_1553754554[df_stamps_acat_ND_gauss_1553754554['stamp_type'] == \"migrad timestamp\"]['timestamp'] - int(_start_st[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, clearly the only dominant part of migrad is FitFCN. New run with timestamps inside there to measure SetFCN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_1553758901 = build_comb_df_split_timing_info('../acat19_1553758901.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_1553758901)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so SetFCN is not the main problem here, though it might be the cause of the two ZMQ threads problem.\n",
    "\n",
    "But, wait a minute there, that's a pretty significant gap over there in the migrad timestamps all of a sudden! `Synchronize` is the major rest-term culprit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_st = df_stamps_acat_1553758901[df_stamps_acat_1553758901['stamp_type'] == \"start migrad\"]\n",
    "df_stamps_acat_1553758901[df_stamps_acat_1553758901['stamp_type'] == \"migrad timestamp\"]['timestamp'] - int(_start_st[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, added some timestamps there as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_1553763665 = build_comb_df_split_timing_info('../acat19_1553763665.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_1553763665)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_st = int(df_stamps_acat_1553763665[df_stamps_acat_1553763665['stamp_type'] == \"start migrad\"][\"timestamp\"])\n",
    "df_stamps_acat_1553763665[df_stamps_acat_1553763665['stamp_type'].str.contains('Grad')][\"timestamp\"] - _start_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, interesting, the first part is the first big loop in `synchronize_parameter_settings`, but the second part is not the second big loop, but the copying of parameters from one list to the other! Both take about the same time and the rest is insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, df_stamps_acat_1553778980 = build_comb_df_split_timing_info('../acat19_1553778980.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, forgot to add worker_id there, again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_1553779882 = build_comb_df_split_timing_info('../acat19_1553779882.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_1553779882)\n",
    "ax.set_xlim(2.01e10, 2.1e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there's definitely something there that's taking a while... But which part is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_differentiate_timestamps_duration_per_task(df, task_ix, worker_id=0):\n",
    "    worker = (df[\"worker_id\"] == worker_id)\n",
    "    _df_w0 = df[worker]\n",
    "    first_task_start = int(_df_w0[_df_w0[\"stamp_type\"].str.contains('job done - started')]['timestamp'].iloc[task_ix])\n",
    "    first_task_end = int(_df_w0[_df_w0[\"stamp_type\"].str.contains('job done - finished')]['timestamp'].iloc[task_ix])\n",
    "    first_task = (_df_w0['timestamp'] > first_task_start) & (_df_w0['timestamp'] < first_task_end)\n",
    "    _st = _df_w0[_df_w0[\"stamp_type\"].str.contains('setup_differentiate') & first_task]['timestamp']\n",
    "    _st = _st - _st.iloc[0]\n",
    "    return _st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worker 0, first task:\n",
    "setup_differentiate_timestamps_duration_per_task(df_stamps_acat_1553779882, 0, worker_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worker 0, second task:\n",
    "setup_differentiate_timestamps_duration_per_task(df_stamps_acat_1553779882, 1, worker_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it's rather clear that most time in the first task goes to the stuff between t6 and t7, which is... as we guessed: the function call! `fVal` has to be calculated once at the start for that set of parameters. But wait, doesn't that hold for all tasks? I think it does.\n",
    "\n",
    "To be sure, let's check out a few more tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_differentiate_fVal_duration(df, task_ix, worker_id=0):\n",
    "    _ts = setup_differentiate_timestamps_duration_per_task(df, task_ix, worker_id=worker_id)\n",
    "    return _ts.iloc[6] - _ts.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all first tasks:\n",
    "print([setup_differentiate_fVal_duration(df_stamps_acat_1553779882, 0, worker_id=w) for w in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 2-10th tasks:\n",
    "print([setup_differentiate_fVal_duration(df_stamps_acat_1553779882, t, worker_id=w) for w in range(8) for t in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 11-100th tasks:\n",
    "print([setup_differentiate_fVal_duration(df_stamps_acat_1553779882, t, worker_id=w) for w in range(8) for t in range(10, 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 101-250th tasks:\n",
    "print([setup_differentiate_fVal_duration(df_stamps_acat_1553779882, t, worker_id=w) for w in range(8) for t in range(100, 250)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, seems pretty solid.\n",
    "\n",
    "Now, what exactly causes this first fVal calculation to be so slow? Recalculation of all cached elements? If so, then you won't get out of this.\n",
    "\n",
    "So then the only option left to make things scale better is to make communication times shorter.\n",
    "\n",
    "## Shorter communication times\n",
    "\n",
    "- The most obvious option to do this is to simply send around less stuff. We could send only the updated gradients (and hessians and stepsizes) that are necessary for a certain task. This could be sent along with the task, for instance. This would save on average `5 * 3 * N_tasks * (N_workers-1)/N_workers` sends, i.e., for our example here, about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 * 3 * 1600 * 7/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more simply put, it reduces the sending workload by a factor `N_workers` for the gradient updates. Unfortunately, there's still the parameter updates which have to be sent fully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 April\n",
    "\n",
    "## Benchmark with added `none_have_been_calculated` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_stamps_acat_1554710451 = build_comb_df_split_timing_info('../acat19_1554710451.out', extract_fcn=load_acat19_out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_1554710451)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, doesn't really seem to help that much, unfortunately... putting it next to an earlier run, it's more or less similar in communication time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_timestamps(df_stamps_acat_1553763665)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
